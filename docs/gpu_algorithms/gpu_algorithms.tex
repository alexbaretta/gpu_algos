\documentclass{amsbook}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{textcomp}

\usepackage{pmboxdraw}
\pmboxdrawsetup{Block/box={\texttt{0}}}

\title{GPU Algorithms: Analysis and Implementation}
\author{Alex Baretta \\
        \texttt{alex@baretta.com}}
\date{\today}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{proposition}{Proposition}[chapter]

\begin{document}

\begin{abstract}
This document anlyzes the GPU algorithms provided by the GPU Algos project. Each chapter examines a specific computational problem and explores multiple implementation strategies, covering mathematical foundations, memory access patterns, and implementation details that achieve optimal performance on modern GPU architectures.

This document is currently woefully incomplete. Currently, the following items are outstanding:
\begin{itemize}
    \item Add sections on TensorCore algorithms for matrix multiplication, including using the GEMM APIs in CUBLASlt and in CUTLASS.
    \item Add a chapter on matrix transpose. The problem is mathematically trivial, but a fast implementation needs to make efficient use of the memory subsystem.
    \item Add a chapter on parallel scans, comparing and contrasting my implementaion with the standard parallel scan documented GPU Gems 3.
    \item Add a chapter on parallel sorting. The project currently only contains an implementation of bitonic sort, but I aim to add others: in particular radix sort and quicksort.
    \item Add a chapter on Generalized Linear Models (GLM), in particular GLMNET, and how the GLM training and evaluation can be implemented efficiently on GPUs.
\end{itemize}
\end{abstract}

\maketitle
\vspace*{\fill}
\begin{center}
    \textcopyright\ 2025 Alex Baretta. All rights reserved.
\end{center}
\vspace*{\fill}

\tableofcontents


\chapter{Matrix Multiplication}

Matrix multiplication is a fundamental operation in linear algebra and computational science. This chapter explores the problem of efficiently computing matrix products on GPU architectures and examines three distinct implementation strategies: naive, warp-based, and tiled approaches.

\section{Problem Definition}

For matrices $A \in \mathbb{R}^{m \times n}$ and $B \in \mathbb{R}^{n \times k}$, the matrix product $C = AB \in \mathbb{R}^{m \times k}$ is defined by:

\begin{equation}
C_{i,j} = \sum_{\ell=1}^{n} A_{i,\ell} \cdot B_{\ell,j}
\end{equation}

for $i = 1, \ldots, m$ and $j = 1, \ldots, k$.

\subsection{Computational Characteristics}

The matrix multiplication problem has the following computational properties:

\begin{itemize}
\item \textbf{Time complexity}: $O(mnk)$ arithmetic operations
\item \textbf{Space complexity}: $O(mn + nk + mk)$ memory requirements
\item \textbf{Arithmetic intensity}: $\frac{2mnk}{4(mn + nk + mk)}$ operations per byte (for 32-bit floats)
\item \textbf{Parallelism}: Each output element $C_{i,j}$ can be computed independently
\end{itemize}

\subsection{Memory Access Challenges}

Computing element $C_{i,j}$ requires accessing:
\begin{itemize}
\item Row $i$ of matrix $A$: $A_{i,1}, A_{i,2}, \ldots, A_{i,n}$
\item Column $j$ of matrix $B$: $B_{1,j}, B_{2,j}, \ldots, B_{n,j}$
\end{itemize}

When matrices are stored in row-major order, accessing columns of matrix $B$ results in poor spatial locality, leading to cache misses and suboptimal memory bandwidth utilization. This fundamental challenge motivates the different implementation strategies explored in this chapter.

\section{Naive Implementation}

The naive approach implements the matrix multiplication definition directly, with each GPU thread computing exactly one output element. This section provides a detailed analysis of the memory access patterns and coalescing behavior.

\subsection{Algorithm Description}

The naive algorithm follows a straightforward "one thread per output element" strategy:

\begin{algorithm}
\caption{Naive Matrix Multiplication}
\begin{algorithmic}[1]
\Require Matrices $A \in \mathbb{R}^{m \times n}$, $B \in \mathbb{R}^{n \times k}$
\Ensure Matrix $C \in \mathbb{R}^{m \times k}$ such that $C = AB$

\For{each thread $(i,j)$ where $0 \leq i < m$ and $0 \leq j < k$}
    \State $\text{sum} \leftarrow 0$
    \For{$\ell = 0$ to $n-1$}
        \State $\text{sum} \leftarrow \text{sum} + A[i][\ell] \times B[\ell][j]$
    \EndFor
    \State $C[i][j] \leftarrow \text{sum}$
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Thread Block Organization}

The implementation uses 2D thread blocks with dimensions $(\text{blockDim.x}, \text{blockDim.y})$. Each thread computes:
\begin{align}
i &= \text{blockIdx.y} \times \text{blockDim.y} + \text{threadIdx.y} \\
j &= \text{blockIdx.x} \times \text{blockDim.x} + \text{threadIdx.x}
\end{align}

The grid dimensions are set to cover the entire output matrix:
\begin{align}
\text{gridDim.x} &= \lceil k/\text{blockDim.x} \rceil \\
\text{gridDim.y} &= \lceil m/\text{blockDim.y} \rceil
\end{align}

\subsection{Memory Coalescing Analysis}

Understanding memory coalescing in the naive implementation requires analyzing how threads within a warp access memory during each iteration of the inner loop.

\subsubsection{Warp Organization}

GPU warps consist of 32 consecutive threads. For a 2D thread block, threads are linearized in row-major order. Consider a thread block of size $(\text{blockDim.x}, \text{blockDim.y})$:

\begin{itemize}
\item Thread $(t_x, t_y)$ has linear thread ID: $\text{tid} = t_y \times \text{blockDim.x} + t_x$
\item Warp $w$ contains threads with IDs $[32w, 32w+31]$
\end{itemize}

For block dimensions $(B_x, B_y)$, warp $w$ contains threads:
\begin{equation}
(t_x, t_y) \text{ where } 32w \leq t_y \times B_x + t_x \leq 32w + 31
\end{equation}

\subsubsection{Memory Access Patterns}

During iteration $\ell$ of the inner loop, each thread $(t_x, t_y)$ accesses:
\begin{itemize}
\item \textbf{Matrix A}: Element $A[i][\ell] = A[\text{blockIdx.y} \times B_y + t_y][\ell]$
\item \textbf{Matrix B}: Element $B[\ell][j] = B[\ell][\text{blockIdx.x} \times B_x + t_x]$
\end{itemize}

\subsubsection{Coalesced Memory Transactions}

A memory transaction is \textbf{coalesced} when consecutive threads in a warp access consecutive memory addresses. Let's analyze each matrix:

\paragraph{Matrix A Access Pattern:}
Threads in the same warp may access different rows of matrix $A$ (different values of $t_y$), but all access the same column $\ell$. The number of distinct rows accessed by a warp depends on the block dimensions.

For a warp containing threads with $t_y$ values in range $[t_{y,\min}, t_{y,\max}]$, the number of distinct memory transactions for matrix $A$ is:
\begin{equation}
\text{Transactions}_A = t_{y,\max} - t_{y,\min} + 1
\end{equation}

\paragraph{Matrix B Access Pattern:}
Threads in the same warp access the same row $\ell$ of matrix $B$, but different columns (different values of $t_x$). Since matrix $B$ is stored in row-major order, consecutive threads access consecutive memory locations, resulting in coalesced access.

For a warp containing threads with $t_x$ values in range $[t_{x,\min}, t_{x,\max}]$, the number of memory transactions for matrix $B$ is:
\begin{equation}
\text{Transactions}_B = \lceil \frac{t_{x,\max} - t_{x,\min} + 1}{32} \rceil
\end{equation}

In most cases, this equals 1 transaction per warp.

\subsubsection{Block Size Optimization Analysis}

Let's analyze specific block configurations to determine optimal memory coalescing:

\paragraph{Case 1: Block size $(16, 16)$}
\begin{itemize}
\item Warp 0: threads $(0,0)$ to $(15,1)$ (threads 0-31)
\item $t_y$ range: $[0, 1]$ → 2 transactions for matrix $A$
\item $t_x$ range: $[0, 15]$ → 1 transaction for matrix $B$
\item \textbf{Total per warp per iteration}: 3 transactions
\end{itemize}

\paragraph{Case 2: Block size $(8, 4)$}
\begin{itemize}
\item Warp 0: threads $(0,0)$ to $(7,3)$ (threads 0-31)
\item $t_y$ range: $[0, 3]$ → 4 transactions for matrix $A$
\item $t_x$ range: $[0, 7]$ → 1 transaction for matrix $B$
\item \textbf{Total per warp per iteration}: 5 transactions
\end{itemize}

\paragraph{Case 3: Block size $(4, 8)$}
\begin{itemize}
\item Warp 0: threads $(0,0)$ to $(3,7)$ (threads 0-31)
\item $t_y$ range: $[0, 7]$ → 8 transactions for matrix $A$
\item $t_x$ range: $[0, 3]$ → 1 transaction for matrix $B$
\item \textbf{Total per warp per iteration}: 9 transactions
\end{itemize}

\paragraph{Case 4: Block size $(32, 1)$}
\begin{itemize}
\item Warp 0: threads $(0,0)$ to $(31,0)$ (threads 0-31)
\item $t_y$ range: $[0, 0]$ → 1 transaction for matrix $A$
\item $t_x$ range: $[0, 31]$ → 1 transaction for matrix $B$
\item \textbf{Total per warp per iteration}: 2 transactions
\end{itemize}

\subsubsection{General Formula}

For a block of size $(B_x, B_y)$, the number of memory transactions per warp per iteration is:

\begin{equation}
\text{Transactions per warp} = \lceil \frac{32}{B_x} \rceil + 1
\end{equation}

where:
\begin{itemize}
\item $\lceil \frac{32}{B_x} \rceil$ accounts for matrix $A$ transactions (number of distinct rows in warp)
\item $1$ accounts for matrix $B$ transactions (coalesced column access)
\end{itemize}

\begin{proposition}
The optimal block configuration for memory coalescing in the naive algorithm is $(32, 1)$, achieving the minimum of 2 transactions per warp per iteration.
\end{proposition}

However, practical considerations such as occupancy, register usage, and cache behavior may favor other configurations like $(16, 16)$ despite the higher transaction count.

\subsection{Performance Characteristics}

The naive implementation serves as a baseline with the following characteristics:
\begin{itemize}
\item \textbf{Arithmetic intensity}: 1 operation per memory access (no data reuse)
\item \textbf{Memory pattern}: Poor spatial locality for matrix $B$
\item \textbf{Simplicity}: Straightforward implementation and debugging
\item \textbf{Scalability}: Naturally handles arbitrary matrix dimensions
\end{itemize}

\section{Warp-Based Implementation}

The warp-based implementation leverages warp shuffle operations to perform reduction within warps, avoiding the need for shared memory while maintaining the one-thread-per-output-element paradigm.

\subsection{Algorithm Description}

Each thread still computes one output element, but the inner loop computation is restructured to use warp shuffle reductions:

\begin{algorithm}
\caption{Warp-Based Matrix Multiplication}
\begin{algorithmic}[1]
\Require Matrices $A \in \mathbb{R}^{m \times n}$, $B \in \mathbb{R}^{n \times k}$
\Ensure Matrix $C \in \mathbb{R}^{m \times k}$ such that $C = AB$

\For{each thread $(i,j)$ where $0 \leq i < m$ and $0 \leq j < k$}
    \State $\text{sum} \leftarrow 0$
    \For{$\ell = 0$ to $n-1$ step $32$}
        \State $\text{partial\_sum} \leftarrow 0$
        \For{$s = 0$ to $\min(31, n-1-\ell)$}
            \State $\text{partial\_sum} \leftarrow \text{partial\_sum} + A[i][\ell+s] \times B[\ell+s][j]$
        \EndFor
        \Comment{Warp shuffle reduction}
        \For{$\text{offset} = 16, 8, 4, 2, 1$}
            \State $\text{partial\_sum} \leftarrow \text{partial\_sum} + \text{\_\_shfl\_down\_sync}(\text{partial\_sum}, \text{offset})$
        \EndFor
        \If{$\text{threadIdx.x} \bmod 32 = 0$}
            \State $\text{sum} \leftarrow \text{sum} + \text{partial\_sum}$
        \EndIf
    \EndFor
    \State $C[i][j] \leftarrow \text{sum}$
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Memory Access Analysis}

The warp-based approach creates different memory access patterns compared to the naive implementation:

\begin{itemize}
\item \textbf{Matrix A}: Each thread in a warp accesses different rows, similar to naive
\item \textbf{Matrix B}: Each thread in a warp accesses different rows (not columns), eliminating coalescing opportunities
\end{itemize}

This trade-off sacrifices memory coalescing for matrix $B$ in exchange for avoiding shared memory usage and enabling warp-level parallelism.

\subsection{Performance Characteristics}

\begin{itemize}
\item \textbf{Advantages}: No shared memory usage, reduced synchronization overhead
\item \textbf{Disadvantages}: Poor memory coalescing for matrix $B$, complex indexing
\item \textbf{Use case}: Experimental exploration of warp shuffle primitives
\end{itemize}

\section{Tiled Implementation}

Building upon the limitations observed in the naive and warp-based approaches, the tiled implementation restructures the computation to maximize data reuse through shared memory caching.

\subsection{Motivation}

The naive implementation suffers from poor arithmetic intensity (1 operation per memory access), while the warp-based approach has memory coalescing issues. The tiled approach addresses both problems by:

\begin{enumerate}
\item Caching frequently accessed data in fast shared memory
\item Organizing computation to maximize data reuse
\item Maintaining coalesced memory access patterns
\end{enumerate}

\subsection{Mathematical Formulation}

The tiling algorithm decomposes matrices into smaller sub-matrices (tiles) that fit in shared memory. Matrix multiplication can be expressed as:

\begin{equation}
C = AB = \sum_{t=1}^{T} A^{(t)} B^{(t)}
\end{equation}

where $A^{(t)}$ and $B^{(t)}$ are corresponding tiles along the shared dimension.

Let $b = 16$ be the tile size. We partition the output matrix $C$ into square tiles of size $b \times b$:

\begin{equation}
C = \begin{bmatrix}
C^{(0,0)} & C^{(0,1)} & \cdots & C^{(0,K-1)} \\
C^{(1,0)} & C^{(1,1)} & \cdots & C^{(1,K-1)} \\
\vdots & \vdots & \ddots & \vdots \\
C^{(M-1,0)} & C^{(M-1,1)} & \cdots & C^{(M-1,K-1)}
\end{bmatrix}
\end{equation}

where $M = \lceil m/b \rceil$ and $K = \lceil k/b \rceil$.

Each output tile $C^{(I,J)}$ is computed as:

\begin{equation}
C^{(I,J)} = \sum_{t=0}^{T-1} A^{(I,t)} B^{(t,J)}
\end{equation}

where $T = \lceil n/b \rceil$ is the number of tiles along the shared dimension.

\subsection{Algorithm Description}

\subsubsection{Threading Strategy and Access Patterns}

The tiled algorithm maintains the "one thread per output value" strategy but fundamentally changes the memory access pattern:

\paragraph{Threading Strategy:} Each thread computes exactly one element $C_{i,j}$ of the output matrix. Thread $(t_x, t_y)$ in block $(I, J)$ computes:
\begin{equation}
C_{I \cdot b + t_y, J \cdot b + t_x} = \sum_{\ell=1}^{n} A_{I \cdot b + t_y, \ell} \cdot B_{\ell, J \cdot b + t_x}
\end{equation}

\paragraph{Memory Access Pattern:} The algorithm processes square $b \times b$ tiles in shared memory, but the overall access pattern creates slivers:

\begin{center}
    \begin{verbatim}
   Matrix A (m×n):                Matrix B (n×k):
┌───────────────────┐          ┌──────┬─────┬──────┐
│                   │          │      │ ███ │      │
├───────────────────┤          │      │ ███ │      │  ←
│███ ███ ... ███ ███│ ← sliver │      │ ... │      │  tiles
├───────────────────┤          │      │ ███ │      │  ←
│                   │          │      │ ███ │      │
└───────────────────┘          └──────┴─────┴──────┘
      ↑ tiles ↑                          ↑ sliver
    \end{verbatim}
\end{center}

Each thread block accesses:
\begin{itemize}
\item \textbf{Matrix A}: Horizontal sliver (fixed rows, all columns across iterations)
\item \textbf{Matrix B}: Vertical sliver (all rows across iterations, fixed columns)
\item \textbf{Matrix C}: True 2D square tile of size $b \times b$
\end{itemize}

\subsubsection{Algorithmic Pseudocode}

\begin{algorithm}
\caption{Tiled Matrix Multiplication}
\begin{algorithmic}[1]
\Require Matrices $A \in \mathbb{R}^{m \times n}$, $B \in \mathbb{R}^{n \times k}$, tile size $b = 16$
\Ensure Matrix $C \in \mathbb{R}^{m \times k}$ such that $C = AB$

\State Initialize $C \leftarrow 0$
\State $T \leftarrow \lceil n/b \rceil$

\For{each thread block $(I,J)$ processing output tile $C^{(I,J)}$}
    \State Initialize accumulator for each thread: $\text{acc} \leftarrow 0$

    \For{$t = 0$ to $T-1$}
        \State Load tile $A^{(I,t)}$ into shared memory
        \State Load tile $B^{(t,J)}$ into shared memory
        \State Synchronize all threads in block

        \For{$\ell = 0$ to $b-1$}
            \State $\text{acc} \leftarrow \text{acc} + A^{(I,t)}[ty][\ell] \times B^{(t,J)}[\ell][tx]$
        \EndFor

        \State Synchronize all threads in block
    \EndFor

    \State Write $\text{acc}$ to global memory: $C^{(I,J)}[ty][tx] \leftarrow \text{acc}$
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Implementation Details}

\subsubsection{Thread Block Organization}

The implementation uses 2D thread blocks of size $16 \times 16$:
\begin{itemize}
\item Each thread block processes exactly one $16 \times 16$ output tile
\item Thread $(t_x, t_y)$ computes element $C^{(I,J)}[t_y][t_x]$
\item Global coordinates: $i = I \times 16 + t_y$, $j = J \times 16 + t_x$
\end{itemize}

\subsubsection{Shared Memory Usage}

Each thread block maintains two shared memory arrays:
\begin{itemize}
\item \texttt{tile\_A[16][16]}: Caches current $A^{(I,t)}$ tile
\item \texttt{tile\_B[16][16]}: Caches current $B^{(t,J)}$ tile
\end{itemize}

\subsubsection{Memory Coalescing}

Cooperative loading ensures coalesced access:
\begin{itemize}
\item \textbf{Loading A}: Thread $(t_x, t_y)$ loads $A[\text{blockIdx.y} \times 16 + t_y][t \times 16 + t_x]$
\item \textbf{Loading B}: Thread $(t_x, t_y)$ loads $B[t \times 16 + t_y][\text{blockIdx.x} \times 16 + t_x]$
\end{itemize}

\subsection{Performance Analysis}

\subsubsection{Arithmetic Intensity Improvement}

For a $b \times b$ tile processing $b^2$ output elements:
\begin{itemize}
\item Global memory loads: $2b^2$ (one tile each of $A$ and $B$)
\item Computation: $b^3$ operations ($b^2$ elements $\times$ $b$ operations each)
\item Arithmetic intensity: $\frac{b^3}{2b^2} = \frac{b}{2}$ operations per memory access
\end{itemize}

\begin{proposition}
The tiled algorithm improves arithmetic intensity by a factor of $\frac{b}{2}$ compared to the naive approach.
\end{proposition}

\subsubsection{Data Reuse Analysis}

Each element loaded into shared memory is reused $b$ times:
\begin{itemize}
\item Elements of tile $A^{(t)}$: reused $b$ times (once for each column of $B^{(t)}$)
\item Elements of tile $B^{(t)}$: reused $b$ times (once for each row of $A^{(t)}$)
\end{itemize}

\subsection{Correctness Proof}

\begin{theorem}
The tiled matrix multiplication algorithm correctly computes $C = AB$.
\end{theorem}

\begin{proof}
For each element $C_{i,j}$, we need to show:
\begin{equation}
C_{i,j} = \sum_{\ell=1}^{n} A_{i,\ell} \cdot B_{\ell,j}
\end{equation}

By the tiling decomposition:
\begin{align}
C_{i,j} &= \sum_{t=1}^{T} \sum_{\ell=1}^{b} A^{(t)}_{i,\ell} \cdot B^{(t)}_{\ell,j} \\
&= \sum_{t=1}^{T} \sum_{\ell=(t-1)b+1}^{\min(tb,n)} A_{i,\ell} \cdot B_{\ell,j} \\
&= \sum_{\ell=1}^{n} A_{i,\ell} \cdot B_{\ell,j}
\end{align}

The equality follows from the fact that tiles partition the range $[1,n]$.
\end{proof}

\subsection{Performance Results}

Our implementation demonstrates:
\begin{itemize}
\item \textbf{Achieved speedup}: $1.19\times$ over naive implementation
\item \textbf{SM throughput}: $94\%$ (near theoretical maximum)
\item \textbf{Memory throughput}: $94\%$ (bandwidth saturated)
\end{itemize}

\section{Chapter Summary}

This chapter examined three approaches to matrix multiplication on GPU architectures:

\begin{enumerate}
\item \textbf{Naive Implementation}: Simple and correct, but suffers from poor arithmetic intensity and suboptimal memory coalescing for certain block configurations.

\item \textbf{Warp-Based Implementation}: Explores warp shuffle primitives but sacrifices memory coalescing for matrix $B$, demonstrating the trade-offs in algorithm design.

\item \textbf{Tiled Implementation}: Achieves optimal performance by maximizing data reuse through shared memory caching while maintaining coalesced memory access patterns.
\end{enumerate}

The progression from naive to tiled implementation illustrates how algorithm design must consider hardware architecture. The detailed memory coalescing analysis for the naive implementation provides the foundation for understanding why more sophisticated approaches like tiling are necessary for optimal GPU performance.

Key insights include:
\begin{itemize}
\item Memory coalescing patterns depend critically on thread block dimensions
\item Arithmetic intensity can be dramatically improved through data reuse
\item Hardware resource utilization (SM and memory throughput) validates algorithmic choices
\item Trade-offs between simplicity and performance guide implementation decisions
\end{itemize}

% Future chapters will go here:
% \chapter{Parallel Reduction}
% \chapter{Convolution Algorithms}
% \chapter{Sorting Algorithms}
% etc.

\end{document}
