\documentclass{amsbook}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{pmboxdraw}
\pmboxdrawsetup{Block/box={\texttt{0}}}

\title{GPU Algorithms: Analysis and Implementation}
\author{Alex Baretta \\
        \texttt{alex@baretta.com}}
\date{\today}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{proposition}{Proposition}[chapter]

\begin{document}

\begin{abstract}
This document provides comprehensive analyses of GPU algorithms for high-performance computing. Each chapter examines a specific algorithm class, covering mathematical foundations, correctness proofs, memory access patterns, and implementation details that achieve optimal performance on modern GPU architectures.
\end{abstract}

\maketitle

\tableofcontents


\chapter{Tiled Matrix Multiplication}

Matrix multiplication is a fundamental operation in linear algebra and computational science. For matrices $A \in \mathbb{R}^{m \times n}$ and $B \in \mathbb{R}^{n \times k}$, the product $C = AB \in \mathbb{R}^{m \times k}$ is defined by:

\begin{equation}
C_{i,j} = \sum_{\ell=1}^{n} A_{i,\ell} \cdot B_{\ell,j}
\end{equation}

for $i = 1, \ldots, m$ and $j = 1, \ldots, k$.

While conceptually straightforward, efficient implementation on modern GPU architectures requires careful consideration of memory hierarchy and parallel execution patterns.

\section{Matrix Multiplication Fundamentals}

\subsection{Computational Complexity}

The standard matrix multiplication algorithm has:
\begin{itemize}
\item \textbf{Time complexity}: $O(mnk)$ arithmetic operations
\item \textbf{Space complexity}: $O(mn + nk + mk)$ memory
\item \textbf{Arithmetic intensity}: $\frac{2mnk}{4(mn + nk + mk)}$ operations per byte (for 32-bit floats)
\end{itemize}

\subsection{Memory Access Patterns}

In the naive implementation, computing element $C_{i,j}$ requires:
\begin{itemize}
\item Row $i$ of matrix $A$: $A_{i,1}, A_{i,2}, \ldots, A_{i,n}$
\item Column $j$ of matrix $B$: $B_{1,j}, B_{2,j}, \ldots, B_{n,j}$
\end{itemize}

This creates poor spatial locality for matrix $B$ when stored in row-major order, leading to cache misses and memory bandwidth limitations.

\section{The Tiling Algorithm}

\subsection{Basic Principle}

The tiling (or blocking) algorithm decomposes the matrices into smaller sub-matrices (tiles) that fit in fast memory (shared memory on GPU). The key insight is that matrix multiplication can be expressed as:

\begin{equation}
C = AB = \sum_{t=1}^{T} A^{(t)} B^{(t)}
\end{equation}

where $A^{(t)}$ and $B^{(t)}$ are corresponding tiles along the shared dimension.

\subsection{Mathematical Formulation}

To implement the tiling principle on GPU architecture efficiently, we use \textbf{2D square tiling} where both the computation and memory access patterns are organized around square sub-matrices.

Let $b = 16$ be the tile size (chosen to match GPU thread block dimensions). We partition the output matrix $C$ into square tiles of size $b \times b$:

\begin{equation}
C = \begin{bmatrix}
C^{(0,0)} & C^{(0,1)} & \cdots & C^{(0,K-1)} \\
C^{(1,0)} & C^{(1,1)} & \cdots & C^{(1,K-1)} \\
\vdots & \vdots & \ddots & \vdots \\
C^{(M-1,0)} & C^{(M-1,1)} & \cdots & C^{(M-1,K-1)}
\end{bmatrix}
\end{equation}

where $M = \lceil m/b \rceil$ and $K = \lceil k/b \rceil$ are the number of tile rows and columns, respectively.

Each output tile $C^{(I,J)}$ corresponds to rows $I \cdot b$ to $(I+1) \cdot b - 1$ and columns $J \cdot b$ to $(J+1) \cdot b - 1$ of the result matrix.

For each output tile $C^{(I,J)}$, we compute:

\begin{equation}
C^{(I,J)} = \sum_{t=0}^{T-1} A^{(I,t)} B^{(t,J)}
\end{equation}

where:
\begin{itemize}
\item $T = \lceil n/b \rceil$ is the number of tiles along the shared dimension
\item $A^{(I,t)}$ is the $b \times b$ tile from matrix $A$ at tile position $(I,t)$
\item $B^{(t,J)}$ is the $b \times b$ tile from matrix $B$ at tile position $(t,J)$
\end{itemize}

More precisely, for tile indices $I$, $J$, and $t$:
\begin{align}
A^{(I,t)} &= A[I b : (I+1) b, \; t b : (t+1) b] \\
B^{(t,J)} &= B[t b : (t+1) b, \; J b : (J+1) b]
\end{align}

This 2D tiling approach offers several advantages over element-wise computation:
\begin{enumerate}
\item Each thread block processes exactly one output tile $C^{(I,J)}$
\item The shared dimension $n$ is processed in chunks of size $b$, enabling data reuse
\item Memory accesses are coalesced and cache-friendly
\item Thread block geometry naturally maps to tile geometry
\end{enumerate}

\subsection{Algorithm Description}

\subsubsection{Threading Strategy and Access Patterns}

The tiled matrix multiplication algorithm maintains the same \textbf{"one thread per output value"} strategy as the naive approach, but fundamentally changes the memory access pattern. Understanding this distinction is crucial:

\paragraph{Threading Strategy:} Each thread computes exactly one element $C_{i,j}$ of the output matrix. Thread $(t_x, t_y)$ in block $(I, J)$ computes:
\begin{equation}
C_{I \cdot b + t_y, J \cdot b + t_x} = \sum_{\ell=1}^{n} A_{I \cdot b + t_y, \ell} \cdot B_{\ell, J \cdot b + t_x}
\end{equation}

\paragraph{Memory Access Pattern:} While we process square $b \times b$ tiles in shared memory during each iteration, the overall access pattern across all iterations creates \textbf{slivers} rather than true 2D tiles:

\begin{center}
    \begin{verbatim}
   Matrix A (m×n):                Matrix B (n×k):
┌───────────────────┐          ┌──────┬─────┬──────┐
│                   │          │      │ ███ │      │
├───────────────────┤          │      │ ███ │      │  ←
│███ ███ ... ███ ███│ ← sliver │      │ ... │      │  tiles
├───────────────────┤          │      │ ███ │      │  ←
│                   │          │      │ ███ │      │
└───────────────────┘          └──────┴─────┴──────┘
      ↑ tiles ↑                          ↑ sliver
    \end{verbatim}
\end{center}

Each thread block accesses:
\begin{itemize}
\item \textbf{Matrix A}: Horizontal sliver (fixed rows $[I \cdot b : (I+1) \cdot b - 1]$, all columns across iterations)
\item \textbf{Matrix B}: Vertical sliver (all rows across iterations, fixed columns $[J \cdot b : (J+1) \cdot b - 1]$)
\item \textbf{Matrix C}: True 2D square tile $C^{(I,J)}$ of size $b \times b$
\end{itemize}

\paragraph{Dual Computation Model:} The algorithm alternates between two phases:
\begin{enumerate}
\item \textbf{Cooperative loading}: All threads collaborate to load square tiles into shared memory
\item \textbf{Independent computation}: Each thread independently computes its output element using cached data
\end{enumerate}

This hybrid approach maximizes data reuse while maintaining the simple one-thread-per-output paradigm.

\subsubsection{Algorithmic Pseudocode}

\begin{algorithm}
\caption{2D Tiled Matrix Multiplication}
\begin{algorithmic}[1]
\Require Matrices $A \in \mathbb{R}^{m \times n}$, $B \in \mathbb{R}^{n \times k}$, tile size $b = 16$
\Ensure Matrix $C \in \mathbb{R}^{m \times k}$ such that $C = AB$

\State Initialize $C \leftarrow 0$
\State $T \leftarrow \lceil n/b \rceil$ \Comment{Number of slivers along shared dimension}

\For{each thread block $(I,J)$ processing output tile $C^{(I,J)}$}
    \State Initialize accumulator for each thread: $\text{acc} \leftarrow 0$

    \For{$t = 0$ to $T-1$} \Comment{Iterate over shared dimension slivers}
        \State Load tile $A^{(I,t)}$ into shared memory \Comment{All threads cooperate}
        \State Load tile $B^{(t,J)}$ into shared memory \Comment{All threads cooperate}
        \State Synchronize all threads in block

        \For{$\ell = 0$ to $b-1$} \Comment{Each thread computes its element}
            \State $\text{acc} \leftarrow \text{acc} + A^{(I,t)}[ty][\ell] \times B^{(t,J)}[\ell][tx]$
        \EndFor

        \State Synchronize all threads in block
    \EndFor

    \State Write $\text{acc}$ to global memory: $C^{(I,J)}[ty][tx] \leftarrow \text{acc}$
\EndFor
\end{algorithmic}
\end{algorithm}

\section{GPU Implementation Details}

\subsection{Thread Block Organization}

Our implementation uses a 2D thread block of size $16 \times 16$, creating a natural one-to-one mapping between thread blocks and output tiles:
\begin{itemize}
\item Each thread block processes exactly one $16 \times 16$ output tile $C^{(I,J)}$
\item Thread $(t_x, t_y)$ within the block computes element $C^{(I,J)}[t_y][t_x]$
\item Global output coordinates: $i = I \times 16 + t_y$ and $j = J \times 16 + t_x$
\end{itemize}

The grid dimensions are set to cover the entire output matrix:
\begin{align}
\text{gridDim.x} &= \lceil k/16 \rceil \quad \text{(number of column tiles)} \\
\text{gridDim.y} &= \lceil m/16 \rceil \quad \text{(number of row tiles)}
\end{align}

\subsection{Shared Memory Usage}

Each thread block maintains two shared memory arrays to cache the current tiles:
\begin{itemize}
\item \texttt{tile\_A[16][16]}: Caches the current $A^{(I,t)}$ tile
\item \texttt{tile\_B[16][16]}: Caches the current $B^{(t,J)}$ tile
\end{itemize}

During iteration $t$, the tiles correspond to:
\begin{align}
\text{tile\_A} &\gets A[I \times 16 : (I+1) \times 16, \; t \times 16 : (t+1) \times 16] \\
\text{tile\_B} &\gets B[t \times 16 : (t+1) \times 16, \; J \times 16 : (J+1) \times 16]
\end{align}

\subsection{Memory Coalescing}

The cooperative loading pattern ensures coalesced memory access:
\begin{itemize}
\item \textbf{Loading matrix A}: Thread $(t_x, t_y)$ loads element at row $(\text{blockIdx.y} \times 16 + t_y)$, column $(t \times 16 + t_x)$
\item \textbf{Loading matrix B}: Thread $(t_x, t_y)$ loads element at row $(t \times 16 + t_y)$, column $(\text{blockIdx.x} \times 16 + t_x)$
\end{itemize}

This ensures that consecutive threads within a warp access consecutive memory locations, maximizing memory bandwidth utilization.

\subsection{Computation Pattern}

Each thread accumulates its result across all tile iterations:
\begin{enumerate}
\item Initialize local accumulator to zero
\item For each shared dimension tile $t = 0, 1, \ldots, T-1$:
\begin{enumerate}
    \item Cooperatively load tiles into shared memory
    \item Synchronize threads (\texttt{\_\_syncthreads()})
    \item Compute partial dot product: $\sum_{\ell=0}^{15} \text{tile\_A}[t_y][\ell] \times \text{tile\_B}[\ell][t_x]$
    \item Synchronize threads before next iteration
\end{enumerate}
\item Write final accumulated result to global memory
\end{enumerate}

\section{Correctness Proof}

\begin{theorem}
The tiled matrix multiplication algorithm correctly computes $C = AB$.
\end{theorem}

\begin{proof}
We need to show that for each element $C_{i,j}$:
\begin{equation}
C_{i,j} = \sum_{\ell=1}^{n} A_{i,\ell} \cdot B_{\ell,j}
\end{equation}

By the tiling decomposition:
\begin{align}
C_{i,j} &= \sum_{t=1}^{T} \sum_{\ell=1}^{b} A^{(t)}_{i,\ell} \cdot B^{(t)}_{\ell,j} \\
&= \sum_{t=1}^{T} \sum_{\ell=(t-1)b+1}^{\min(tb,n)} A_{i,\ell} \cdot B_{\ell,j} \\
&= \sum_{\ell=1}^{n} A_{i,\ell} \cdot B_{\ell,j}
\end{align}

The second equality follows from the definition of tiles, and the third from the fact that the tiles partition the range $[1,n]$.
\end{proof}

\section{Memory Access Analysis}

\subsection{Naive Algorithm Memory Complexity}

For computing element $C_{i,j}$, the naive algorithm requires:
\begin{itemize}
\item $n$ loads from global memory (row of $A$ + column of $B$)
\item $n$ floating-point operations
\item Arithmetic intensity: $\frac{n}{n} = 1$ operation per memory access
\end{itemize}

\subsection{Tiled Algorithm Memory Complexity}

For a $b \times b$ tile processing $b^2$ output elements:
\begin{itemize}
\item Global memory loads: $2b^2$ (one tile of $A$ and one of $B$)
\item Computation: $b^3$ operations ($b^2$ elements $\times$ $b$ operations each)
\item Arithmetic intensity: $\frac{b^3}{2b^2} = \frac{b}{2}$ operations per memory access
\end{itemize}

\begin{proposition}
The tiled algorithm improves arithmetic intensity by a factor of $\frac{b}{2}$ compared to the naive approach.
\end{proposition}

\subsection{Cache Reuse Analysis}

Each element loaded into shared memory is reused:
\begin{itemize}
\item Elements of tile $A^{(t)}$: reused $b$ times (once for each column of $B^{(t)}$)
\item Elements of tile $B^{(t)}$: reused $b$ times (once for each row of $A^{(t)}$)
\end{itemize}

This $b$-fold reuse reduces effective memory bandwidth requirements.

\section{Performance Benefits}

\subsection{Theoretical Speedup}

Assuming memory bandwidth is the bottleneck, the theoretical speedup is:
\begin{equation}
\text{Speedup} = \frac{\text{Memory accesses}_{\text{naive}}}{\text{Memory accesses}_{\text{tiled}}} = \frac{n \cdot mk}{2mk \cdot T} = \frac{n}{2T}
\end{equation}

where $T = \lceil n/b \rceil$.

For $n \gg b$, this approaches $\frac{b}{2}$.

\subsection{Practical Considerations}

Real-world performance depends on:
\begin{itemize}
\item \textbf{Memory hierarchy}: L1 cache, L2 cache, shared memory bandwidth
\item \textbf{Occupancy}: Number of active threads per streaming multiprocessor
\item \textbf{Synchronization overhead}: Cost of \texttt{\_\_syncthreads()} calls
\item \textbf{Bank conflicts}: Shared memory access patterns
\end{itemize}

\section{Implementation Analysis}

Our CUDA implementation demonstrates:
\begin{itemize}
\item \textbf{Achieved speedup}: $1.19\times$ over naive implementation
\item \textbf{SM throughput}: $94\%$ (near theoretical maximum)
\item \textbf{Memory throughput}: $94\%$ (bandwidth saturated)
\end{itemize}

The high throughput values indicate that our implementation successfully saturates both compute and memory resources, achieving near-optimal performance within the constraints of the tiling approach.

\section{Chapter Summary}

The tiled matrix multiplication algorithm demonstrates how algorithm design must consider hardware architecture. By restructuring memory access patterns to exploit the GPU's memory hierarchy, we achieve:

\begin{enumerate}
\item \textbf{Improved arithmetic intensity} from $O(1)$ to $O(b)$
\item \textbf{Better cache utilization} through data reuse
\item \textbf{Coalesced memory access} patterns
\item \textbf{Saturated hardware resources} as confirmed by profiling
\end{enumerate}

The theoretical framework presented here provides the foundation for understanding why and how much tiling algorithms improve performance, while the implementation details show how these principles translate to actual GPU code.

% Future chapters will go here:
% \chapter{Parallel Reduction}
% \chapter{Convolution Algorithms}
% \chapter{Sorting Algorithms}
% etc.

\end{document}
