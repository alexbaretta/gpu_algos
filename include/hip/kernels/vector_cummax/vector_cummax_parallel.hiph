/*
    Copyright (c) 2025 Alessandro Baretta <alex@baretta.com>

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.
*/


// source path: include/hip/kernels/vector_cummax/vector_cummax_parallel.hiph

#pragma once
#include <iostream>
#include <hip/hip_runtime.h>
#include <cxxopts.hpp>
#include <Eigen/Dense>
#include <algorithm>
#include <cassert>

#include "hip/hip_utils.hiph"
#include "hip/kernel_api/vector_1in_1out.hiph"
#include "hip/type_traits.hiph"
#include "hip/check_errors.hiph"


template <HIP_scalar Number>
__global__ void vector_cummax_write_back_parallel(
    Number* prev_result,
    const long prev_n_elems,  // size of vector
    Number* curr_result,
    const long curr_n_elems,  // size of vector
    const long stride_A
) {
    __shared__ Number shm[1]; // This will contain the last value of the previous block

    // We have to update our block of prev_result with the final value from the previous block

    // tid_xxx represent the index of the thread within grid, block, and warp
    const long bid_grid = blockIdx.x;
    const unsigned short tid_block = threadIdx.x;
    const long tid_grid = threadIdx.x + blockIdx.x * blockDim.x;
    // printf("tid_grid: %d, n: %d, blockDim.x: %d, gridDim.x: %d\n", tid_grid, n, blockDim.x, gridDim.x);

    if (tid_block == 0) {
        // This is the first thread in the block.
        // We need to read the element from curr_result that corresponds to this block,
        const Number curr_bid_minus_1_value = (bid_grid > 0) ? curr_result[bid_grid-1] : Number(0);
        shm[0] = curr_bid_minus_1_value;
    }

    __syncthreads();

    if (tid_grid < prev_n_elems) {
        const Number prev_value = prev_result[tid_grid];
        const Number updated_value = hip_max(prev_value, shm[0]);
        prev_result[tid_grid] = updated_value;
    }
}


template <HIP_scalar Number>
__global__ void vector_cummax_by_blocks_parallel(
    const Number* A,
    Number* C,
    const int n,  // size of vector
    const int stride_A,
    const int source_array_size = -1  // size of source array for bounds checking
) {
    // for writing, index this using `wid_block` (warp id)
    Number* shm = static_cast<Number*>(get_dynamic_shared_memory(alignof(Number)));

    // const long n_blocks = gridDim.x;

    // tid_xxx represent the index of the thread within grid, block, and warp
    // const long bid_grid = blockIdx.x;
    // const unsigned short tid_block = threadIdx.x;
    const long tid_grid = threadIdx.x + blockIdx.x * blockDim.x;
    // printf("tid_grid: %d, n: %d, blockDim.x: %d, gridDim.x: %d\n", tid_grid, n, blockDim.x, gridDim.x);

    Number value;
    if constexpr (std::is_floating_point_v<Number>) {
        value = DEVICE_NAN;
    } else {
        value = Number(0);
    }

    // bid_grid (block ID relative to the whole grid) can be >= n_blocks when we call ourselves
    // recursively on a reduced dataset. In this case, we can skip directly to the synchronization,
    const unsigned short tid_warp = threadIdx.x % WARP_SIZE;
    const unsigned short wid_block = threadIdx.x / WARP_SIZE;
    const unsigned short n_warps_per_block = blockDim.x / WARP_SIZE;

    // the size of shm must be at least the number of warps in the block

    // Load data, only if index is within bounds
    if (tid_grid < n) {
        const long index = (tid_grid + 1) * stride_A - 1;
        const long actual_index = (source_array_size > 0) ?
            min(index, (long)(source_array_size - 1)) : index;
        value = A[actual_index];
    }

    // Scan warp using warp-shuffle
    // Each warp produces a scan of 2*WARP_SIZE values
    // (stored in the `value` register of each of the threads of the warp)
    // Initialize: subtree_size = 1, subtree_index = tid_warp (lane)
    // At each step:
    //   shufl value from highest lane of previous subtree: from_lane = std::max(0, (subtree_id-1) * subtree_size)
    //   if (subtree_id % 2 == 1): add shuffled value to local value, offset*=2
    //   all threads: double scanned_size, halve subtree_id


    // Scan warp using warp-shuffle
    for (int subtree_size = 1, subtree_id = tid_warp;
            subtree_size < WARP_SIZE;
            subtree_size <<= 1, subtree_id /= 2) {
        const int from_lane = max(0, subtree_id * subtree_size - 1);
        const Number received_value = __shfl_sync(__activemask(), value, from_lane);
        if (subtree_id % 2 == 1) {
            value = hip_max(value, received_value);
        }
    }

    if (n_warps_per_block == 1) {
        // Only one warp. We have already compute the result. We just need to copy it to C
        if (tid_grid < n) {
            C[tid_grid] = value;
        }
    } else {
        // n_warps_per_block > 1
        // We need to do a warp-shuffle scan across the warps. We will use shared memory
        // to allow each warp within a block to send it terminal value to the master warp
        // which will then perform the scan.

        // Now each warp's values contain the cummaxs for the warp. The last lane of the warp contains the warp total.
        if (tid_warp == LAST_LANE) {
            shm[wid_block] = value;
        }

        __syncthreads(); // Now shm contains input data for the block-level warp-shuffle scan

        // Now the warp totals live in shm. We need to scan shm to compute the block-level scan.
        // We use the same algorithm as above, but we execute with only one warp,
        // as the shared memory size is equal to blockSize / warpSize < warpSize
        assert(get_n_warps_per_block() <= warpSize);
        if (wid_block == 0) {
            // We pick warp 0 to perform the warp-shuffle scan on shared memory.
            Number shm_value = 0;
            if (tid_warp < n_warps_per_block) {
                shm_value = shm[tid_warp]; // Read the final value from the previous warp
            }
            for (int subtree_size = 1, subtree_id = tid_warp;
                subtree_size < WARP_SIZE;
                subtree_size <<= 1, subtree_id /= 2) {
                const int from_lane = max(0, subtree_id * subtree_size - 1);
                const Number received_value = __shfl_sync(__activemask(), shm_value, from_lane);
                if (subtree_id % 2 == 1) {
                    shm_value = hip_max(shm_value, received_value);
                }
            }
            shm[tid_warp] = shm_value;
        }

        __syncthreads(); // Now shm contains output data from the block-level warp-shuffle scan

        // We need to read the final value of wid into wid+1 and update all the values in the warp
        // We use a shared memory broadcast to do this: each thread within a warp reads the same
        // shared memory location: shm[wid_block-1]
        if (wid_block > 0) {
            Number wid_minus_1_value = shm[wid_block-1];
            value = hip_max(value, wid_minus_1_value);
        }
        // For wid_block == 0 (first warp), value is already correct - no combination needed

        // Now the `value` variables contain the the scanned values: we need to write them back to C
        if (tid_grid < n) {
            C[tid_grid] = value;
        }
    }
}

struct Vector_cummax_parallel_spec {
    const hipDeviceProp_t device_prop_;

    const std::string type_;

    const long m_;    // unused for vector cummax
    const long n_;    // size of vector
    const long k_;    // unused for vector cummax

    const long n_A_;

    const long n_C_;

    const long n_temp_;

    const dim3 block_dim_;
    const dim3 grid_dim_;
    const size_t dynamic_shared_mem_words_ = 0;

    constexpr static int DEFAULT_M = 0;    // unused
    constexpr static int DEFAULT_N = 3000; // size of vector
    constexpr static int DEFAULT_K = 0;    // unused
    constexpr static int DEFAULT_BLOCK_DIM_X = 1024;

    inline static void add_kernel_spec_options(cxxopts::Options& options) {
        options.add_options()
            ("N", "Size of vector", cxxopts::value<long>()->default_value(std::to_string(DEFAULT_N)))
            ("block-dim", "Number of threads in the x dimension per block", cxxopts::value<unsigned>()->default_value(std::to_string(DEFAULT_BLOCK_DIM_X)))
            ("type", "Numeric type (half, single/float, double, int<n>, uint<n>)", cxxopts::value<std::string>()->default_value("float"));
        ;
    }

    inline static Vector_cummax_parallel_spec make(
        const cxxopts::ParseResult& options_parsed
    ) {
        // Validate the type option
        const auto& type = options_parsed["type"].as<std::string>();
        if (type != "half" && type != "single" && type != "float" && type != "double" && type != "int8" && type != "int16" && type != "int32" && type != "int64" && type != "uint8" && type != "uint16" && type != "uint32" && type != "uint64") {
            std::cerr << "[ERROR] --type must be one of: half, single/float, double, int<n>, uint<n>" << std::endl;
            throw cxxopts::exceptions::exception("Invalid --type: " + type);
        }
        const auto size = options_parsed["N"].as<long>();
        const auto block_dim_option = options_parsed["block-dim"].as<unsigned>();
        const auto warp_size = get_warp_size();
        const auto block_size = (std::min(size, (long)block_dim_option)  + warp_size - 1) / warp_size * warp_size;
        return Vector_cummax_parallel_spec(
            type,
            size,
            block_size
        );
    }

    int compute_size_of_temp(const int n_elems, const int block_size) {
        assert(block_size > 1);
        if (n_elems <= block_size) {
            return 0;
        }
        int size_temp = 1;
        for(int n_elems_remaining = (n_elems + block_size - 1) / block_size;
            n_elems_remaining > 1;
            n_elems_remaining = (n_elems_remaining + block_size - 1) / block_size
            ) {
            size_temp += n_elems_remaining;
        }
        const int n_blocks = (size_temp + block_size - 1) / block_size;
        return n_blocks * block_size;
    }

    Vector_cummax_parallel_spec(
        const std::string& type,
        const long size,
        const long block_size
    ) : device_prop_(get_default_device_prop()),
        type_(type),
        m_(0),  // unused
        n_(size),
        k_(0),  // unused
        n_A_(size),
        n_C_(size),
        n_temp_(compute_size_of_temp(size, block_size)),
        block_dim_(block_size),
        grid_dim_((size + block_size - 1) / block_size)
    {}
};

static_assert(Check_vector_kernel_spec_1In_1Out<Vector_cummax_parallel_spec>::check_passed, "Vector_cummax_parallel_spec is not a valid kernel spec");


template <HIP_scalar Number_>
class Vector_cummax_parallel_kernel {
    public:
    using Number = Number_;
    using Kernel_spec = Vector_cummax_parallel_spec;

    const Kernel_spec spec_;
    const size_t shared_mem_size_;

    Vector_cummax_parallel_kernel(
        const Kernel_spec spec
    ) : spec_(spec),
        shared_mem_size_((compute_n_warps_per_block(spec_.block_dim_)+1) * sizeof(Number))
        {}

    void block_strided_pass(
        Number* const input_buffer,
        const int prev_result_index,
        const int prev_n_elems,
        Number* const output_buffer,
        const int curr_result_index,
        hipStream_t stream
    ) {
        Number* const prev_result = input_buffer + prev_result_index;
        const int prev_n_blocks = (prev_n_elems + spec_.block_dim_.x - 1)/spec_.block_dim_.x;
        Number* const curr_result = output_buffer + curr_result_index;
        const int curr_n_elems = prev_n_blocks;
        const int curr_n_blocks = (curr_n_elems + spec_.block_dim_.x - 1)/spec_.block_dim_.x;
        vector_cummax_by_blocks_parallel<<<curr_n_blocks, spec_.block_dim_, shared_mem_size_, stream>>>(
            prev_result,
            curr_result,
            curr_n_elems,
            spec_.block_dim_.x,
            prev_n_elems
        );
        if (prev_n_elems > static_cast<int>(spec_.block_dim_.x)) {
            const auto next_result_index = curr_result_index + curr_n_elems;
            block_strided_pass(output_buffer, curr_result_index, curr_n_elems, output_buffer, next_result_index, stream);
            // By the powers of recursion, curr_result is fully scanned
            // Now we have to compute the deltas between curr_result and the final elements
            // of each stride of prev_result, and apply those delta retroactively to the strides
            // of prev_result

            vector_cummax_write_back_parallel<<<prev_n_blocks, spec_.block_dim_, 0, stream>>>(
                prev_result,
                prev_n_elems,
                curr_result,
                curr_n_elems,
                spec_.block_dim_.x
            );
        }
    }

    void run_device_kernel(
        const Number* const gpu_data_A,
        Number* const gpu_data_C,
        Number* const gpu_data_temp,
        hipStream_t stream
    ) {
        // int max_block_size = 0;
        // int opt_grid_size = 0;
        // int max_active_blocks_per_multiprocessor = 0;
        // hip_check_error(hipOccupancyMaxPotentialBlockSize(
        //     &max_block_size,
        //     &opt_grid_size,
        //     vector_cummax_by_blocks_parallel<Number>,
        //     shared_mem_size_,
        //     0
        // ), "hipOccupancyMaxPotentialBlockSize");
        // hip_check_error(hipOccupancyMaxActiveBlocksPerMultiprocessor(
        //     &max_active_blocks_per_multiprocessor,
        //     vector_cummax_by_blocks_parallel<Number>,
        //     max_block_size,
        //     shared_mem_size_
        // ), "hipOccupancyMaxActiveBlocksPerMultiprocessor");
        // block_dim_ = dim3(max_block_size);
        // grid_dim_ = dim3(opt_grid_size);
        // std::cout << "[INFO] max_active_blocks_per_multiprocessor: " << max_active_blocks_per_multiprocessor
        //           << " at block_size:" << max_block_size << std::endl;
        // std::cout << "[INFO] max_block_size: " << max_block_size << std::endl;
        // std::cout << "[INFO] opt_grid_size: " << opt_grid_size << std::endl;

        std::cout << "[INFO] grid_dim_: " << spec_.grid_dim_.x << ", " << spec_.grid_dim_.y << ", " << spec_.grid_dim_.z << std::endl;
        std::cout << "[INFO] block_dim_: " << spec_.block_dim_.x << ", " << spec_.block_dim_.y << ", " << spec_.block_dim_.z << std::endl;
        std::cout << "[INFO] shared_mem_size_: " << shared_mem_size_ << std::endl;

        // In our downward iteration we start by processing A block-wise.
        // The produces in C an array of block-wise cummaxs that we can further process with a stride = block_size.
        // This downward iteration ends when the number of blocks is 1, which means that the result is the
        // global cummax.
        vector_cummax_by_blocks_parallel<<<spec_.grid_dim_, spec_.block_dim_, shared_mem_size_, stream>>>(
            gpu_data_A,
            gpu_data_C,
            spec_.n_,
            1
        );

        auto& prev_result = gpu_data_C;
        auto& curr_result = gpu_data_temp;
        auto& prev_n_elems = spec_.n_;
        if (prev_n_elems > static_cast<int>(spec_.block_dim_.x)) {
            block_strided_pass(prev_result, 0, spec_.n_, curr_result, 0, stream);
        }
    }

    Eigen::Vector<Number, Eigen::Dynamic> run_host_kernel(
        const Eigen::Map<Eigen::Vector<Number, Eigen::Dynamic>>& A
    ) {
        // Compute cumulative max for a vector (treat matrix as a flattened vector)
        Eigen::Vector<Number, Eigen::Dynamic> result(A.rows(), A.cols());
        Number accu = A(0);
        result(0) = accu;
        for (int i = 1; i < A.size(); ++i) {
            accu = std::max(accu, A(i));
            result(i) = accu;
        }
        return result;
    }
};
static_assert(Check_vector_kernel_1In_1Out_template<Vector_cummax_parallel_kernel>::check_passed, "Vector_cummax_parallel is not a valid kernel template");
