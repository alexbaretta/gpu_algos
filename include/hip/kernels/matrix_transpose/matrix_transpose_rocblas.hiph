// Copyright (c) 2025 Alessandro Baretta
// All rights reserved.

// source path: include/hip/kernels/matrix_transpose/matrix_transpose_rocblas.hpp

#pragma once
#include <hip/hip_runtime.h>
#include <rocblas/rocblas.h>
#include <string>
#include <iostream>
#include <cxxopts.hpp>
#include <Eigen/Dense>
#include <hip/hip_fp16.h>

#include "hip/kernel_api/matrix_1in_1out.hiph"
#include "hip/type_traits.hiph"

struct Matrix_transpose_rocblas_spec {
    const std::string type_;

    const long m_;    // Rows of input matrix, cols of output matrix
    const long n_;    // Columns of input matrix, rows of output matrix
    constexpr static long k_ = 0;  // unused

    const long n_rows_A_;
    const long n_cols_A_;

    const long n_rows_C_;
    const long n_cols_C_;

    const long n_rows_temp_;
    const long n_cols_temp_;

    // Note: block_dim and grid_dim are not used with rocBLAS but kept for compatibility
    const dim3 block_dim_;
    const dim3 grid_dim_;
    const size_t dynamic_shared_mem_words_;

    constexpr static int DEFAULT_M = 3000; // rows of A, cols of C
    constexpr static int DEFAULT_N = 300;  // cols of A, rows of C
    constexpr static int DEFAULT_K = 1000; // unused
    constexpr static int DEFAULT_BLOCK_DIM_X = 16;
    constexpr static int DEFAULT_BLOCK_DIM_Y = 16;

    inline static void add_kernel_spec_options(cxxopts::Options& options) {
        options.add_options()
            ("m", "Number of rows in input matrix", cxxopts::value<long>()->default_value(std::to_string(DEFAULT_M)))
            ("n", "Number of columns in input matrix", cxxopts::value<long>()->default_value(std::to_string(DEFAULT_N)))
            ("k", "Unused", cxxopts::value<long>()->default_value(std::to_string(DEFAULT_K)))
            ("block-dim-x,x", "Number of threads in the x dimension per block", cxxopts::value<long>()->default_value(std::to_string(DEFAULT_BLOCK_DIM_X)))
            ("block-dim-y,y", "Number of threads in the y dimension per block", cxxopts::value<long>()->default_value(std::to_string(DEFAULT_BLOCK_DIM_Y)))
            ("type", "Numeric type (half, single/float, double, int<n>, uint<n>)", cxxopts::value<std::string>()->default_value("float"));
        ;
    }

    inline static Matrix_transpose_rocblas_spec make(
        const cxxopts::ParseResult& options_parsed
    ) {
        // Validate the type option
        const auto& type = options_parsed["type"].as<std::string>();
        if (type != "half" && type != "single" && type != "float" && type != "double" && type != "int8" && type != "int16" && type != "int32" && type != "int64" && type != "uint8" && type != "uint16" && type != "uint32" && type != "uint64") {
            std::cerr << "[ERROR] --type must be one of: half, single/float, double, int<n>, uint<n>" << std::endl;
            throw cxxopts::exceptions::exception("Invalid --type: " + type);
        }
        return Matrix_transpose_rocblas_spec(
            type,
            options_parsed["m"].as<long>(),
            options_parsed["n"].as<long>(),
            options_parsed["block-dim-x"].as<long>(),
            options_parsed["block-dim-y"].as<long>()
        );
    }

    inline Matrix_transpose_rocblas_spec(
        const std::string& type,
        const long m,
        const long n,
        const long block_dim_x,
        const long block_dim_y
    ) : type_(type),
        m_(m),
        n_(n),
        n_rows_A_(m),
        n_cols_A_(n),
        n_rows_C_(n),
        n_cols_C_(m),
        n_rows_temp_(0),
        n_cols_temp_(0),
        block_dim_(block_dim_x, block_dim_y),
        grid_dim_(
            (n_ + block_dim_.x - 1) / block_dim_.x,
            (m_ + block_dim_.y - 1) / block_dim_.y
        ),
        dynamic_shared_mem_words_(0)
    {}
};

static_assert(Check_matrix_kernel_spec_1In_1Out<Matrix_transpose_rocblas_spec>::check_passed, "Matrix_transpose_rocblas_spec is not a valid kernel spec");


template <HIP_scalar Number_>
class Matrix_transpose_rocblas_kernel {
    public:
    using Number = Number_;
    using Kernel_spec = Matrix_transpose_rocblas_spec;

    const Kernel_spec spec_;
    rocblas_handle rocblas_handle_;

    Matrix_transpose_rocblas_kernel(
        const Kernel_spec spec
    ) : spec_(spec) {
        // Initialize rocBLAS handle
        rocblas_create_handle(&rocblas_handle_);

        // Note: AMD rocBLAS doesn't have separate math modes like NVIDIA
    }

    ~Matrix_transpose_rocblas_kernel() {
        if (rocblas_handle_) {
            rocblas_destroy_handle(rocblas_handle_);
        }
    }

    void run_device_kernel(
        const Number* const gpu_data_A,
        Number* const gpu_data_C,
        Number* const gpu_data_temp,
        hipStream_t stream
    ) {
        // Set the stream for rocBLAS operations
        rocblas_set_stream(rocblas_handle_, stream);

        const Number alpha = static_cast<Number>(1.0);
        const Number beta = static_cast<Number>(0.0);

        // Matrix transpose using rocBLAS geam
        // For row-major matrices: A is m×n, C is n×m
        // rocBLAS expects row-major, so we need to account for this

        if constexpr (std::is_same_v<Number, float>) {
            // For single precision, use rocblas_sgeam
            // C = alpha * A^T + beta * 0
            // Since we're working with row-major data but rocBLAS expects row-major:
            // - A (m×n row-major) appears as A^T (n×m row-major) to rocBLAS
            // - We want C = A^T, so C (n×m row-major) appears as C^T (m×n row-major) to rocBLAS
            // - So rocBLAS should compute: C^T = (A^T)^T = A
            rocblas_sgeam(rocblas_handle_,
                       rocblas_operation_transpose, rocblas_operation_transpose,   // transpose the input to undo the implicit transpose
                       spec_.m_, spec_.n_,          // output dimensions as seen by rocBLAS (C^T is m×n)
                       &alpha,
                       gpu_data_A, spec_.n_,        // A with leading dimension n (as row-major m×n)
                       &beta,
                       gpu_data_A, spec_.n_,        // dummy (beta=0)
                       gpu_data_C, spec_.m_);       // C with leading dimension m (as row-major n×m)
        } else if constexpr (std::is_same_v<Number, double>) {
            // For double precision, use rocblas_dgeam
            rocblas_dgeam(rocblas_handle_,
                       rocblas_operation_transpose, rocblas_operation_transpose,   // transpose the input to undo the implicit transpose
                       spec_.m_, spec_.n_,          // output dimensions as seen by rocBLAS (C^T is m×n)
                       &alpha,
                       gpu_data_A, spec_.n_,        // A with leading dimension n
                       &beta,
                       gpu_data_A, spec_.n_,        // dummy (beta=0)
                       gpu_data_C, spec_.m_);       // C with leading dimension m
        } else {
            // rocBLAS doesn't provide rocblas_hgeam function for half precision
            std::cerr << "Error: " << spec_.type_ << " is not supported by rocBLAS geam" << std::endl;
            throw std::runtime_error(spec_.type_ + " is not supported by rocBLAS geam");
        }
    }

    Eigen::Matrix<Number, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor> run_host_kernel(
        const Eigen::Map<Eigen::Matrix<Number, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>>& A
    ) {
        return A.transpose().eval();
    }

};
static_assert(Check_matrix_kernel_1In_1Out_template<Matrix_transpose_rocblas_kernel>::check_passed, "Matrix_transpose_rocblas is not a valid kernel template");
