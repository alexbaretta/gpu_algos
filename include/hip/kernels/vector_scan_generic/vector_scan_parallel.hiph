/*
    Copyright (c) 2025 Alessandro Baretta <alex@baretta.com>

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.
*/


// source path: include/hip/kernels/vector_scan_generic/vector_scan_parallel.hiph

#pragma once
#include <cmath>
#include <iostream>
#include <hip/hip_runtime.h>
#include <cxxopts.hpp>
#include <Eigen/Dense>
#include <cassert>
#include <concepts>
#include <utility>
#include <type_traits>

#include "hip/kernel_api/vector_1in_1out.hiph"
#include "hip/hip_utils.hiph"
#include "hip/type_traits.hiph"
#include "hip/check_errors.hiph"


template <typename Number>
Number sum(Number a, Number b) {
    return a + b;
}

template <typename Operation>
concept OPERATION = requires(typename Operation::Number a, typename Operation::Number b) {
    { Operation::apply(a, b) } -> std::same_as<typename Operation::Number>;
    requires std::is_empty_v<Operation>;
};

template <HIP_scalar Number, OPERATION Op>
__global__ void vector_scan_write_back_parallel(
    Number* prev_result,
    const long prev_n_elems,  // size of vector
    Number* curr_result,
    const long curr_n_elems,  // size of vector
    const long stride_A
) {
    __shared__ Number shm[1]; // This will contain the last value of the previous block

    // We have to update our block of prev_result with the final value from the previous block

    // tid_xxx represent the index of the thread within grid, block, and warp
    const auto tid_block = threadIdx.x;
    const long tid_grid = long(threadIdx.x) + long(blockIdx.x) * long(blockDim.x);
    // printf("tid_grid: %d, n: %d, blockDim.x: %d, gridDim.x: %d\n", tid_grid, n, blockDim.x, gridDim.x);

    if (tid_block == 0) {
        // This is the first thread in the block.
        // We read from curr_result the last element of this block in prev_result
        const Number curr_bid_minus_1_value = curr_result[blockIdx.x];
        shm[0] = curr_bid_minus_1_value;
    }

    __syncthreads();

    // We use the last element of this block to update the next block
    const auto write_to_index = tid_grid + blockDim.x;
    if (write_to_index < prev_n_elems) {
        const Number prev_value = prev_result[write_to_index];
        const Number updated_value = Op::apply(shm[0], prev_value);
        prev_result[write_to_index] = updated_value;
    }
}


template <HIP_scalar Number, OPERATION Operation>
__global__ void vector_scan_by_blocks_parallel(
    const Number* A,
    Number* C,
    const long n,  // size of vector
    const long stride_A,
    const long source_array_size = -1  // size of source array for bounds checking
) {
    // for writing, index this using `wid_block` (warp id)
    Number* shm = get_dynamic_shm<Number>();

    // const long n_blocks = gridDim.x;

    // tid_xxx represent the index of the thread within grid, block, and warp
    // const long bid_grid = blockIdx.x;
    // const unsigned short tid_block = threadIdx.x;
    const long tid_grid = long(threadIdx.x) + long(blockIdx.x) * long(blockDim.x);
    // printf("tid_grid: %d, n: %d, blockDim.x: %d, gridDim.x: %d\n", tid_grid, n, blockDim.x, gridDim.x);

    Number value;
    if constexpr (std::is_floating_point_v<Number>) {
        value = DEVICE_NAN;
    } else {
        value = Number(0);
    }

    const unsigned short tid_warp = threadIdx.x % WARP_SIZE;
    const unsigned short wid_block = threadIdx.x / WARP_SIZE;
    const unsigned short n_warps_per_block = blockDim.x / WARP_SIZE;

    // the size of shm must be at least the number of warps in the block

    // Load data, only if index is within bounds
    if (tid_grid < n) {
        const long index = (tid_grid + 1) * stride_A - 1;
        const long actual_index = (source_array_size > 0) ?
            min(index, (long)(source_array_size - 1)) : index;
        value = A[actual_index];
    }

    // Scan warp using warp-shuffle
    // Each warp produces a scan of 2*WARP_SIZE values
    // (stored in the `value` register of each of the threads of the warp)
    // Initialize: subtree_size = 1, subtree_index = tid_warp (lane)
    // At each step:
    //   shufl value from highest lane of previous subtree: from_lane = std::max(0, (subtree_id-1) * subtree_size)
    //   if (subtree_id % 2 == 1): add shuffled value to local value, offset*=2
    //   all threads: double scanned_size, halve subtree_id


    // Scan warp using warp-shuffle
    for (int subtree_size = 1, subtree_id = tid_warp;
            subtree_size < WARP_SIZE;
            subtree_size <<= 1, subtree_id /= 2) {
        const int from_lane = max(0, subtree_id * subtree_size - 1);
        const Number received_value = __shfl_sync(__activemask(), value, from_lane);
        if (subtree_id % 2 == 1) {
            const auto computed_value = Operation::apply(received_value, value);
            #ifdef DEBUG
            if constexpr (std::is_floating_point_v<Number>) {
                if (std::isnan(computed_value)
                    && !std::isnan(value)
                    && !std::isnan(received_value)
                ) {
                    printf("warp-level operation result %f is nan when operands %f and %f\n",
                        float(computed_value), float(value), float(received_value));
                }
            }
            #endif // DEBUG
            value = computed_value;
        }
    }

    if (n_warps_per_block == 1) {
        // Only one warp. We have already compute the result. We just need to copy it to C
        if (tid_grid < n) {
            C[tid_grid] = value;
        }
    } else {
        // n_warps_per_block > 1
        // We need to do a warp-shuffle scan across the warps. We will use shared memory
        // to allow each warp within a block to send it terminal value to the master warp
        // which will then perform the scan.

        // Now each warp's values contain the scans for the warp. The last lane of the warp contains the warp total.
        if (tid_warp == LAST_LANE) {
            shm[wid_block] = value;
        }

        __syncthreads(); // Now shm contains input data for the block-level warp-shuffle scan

        // Now the warp totals live in shm. We need to scan shm to compute the block-level scan.
        // We use the same algorithm as above, but we execute with only one warp,
        // as the shared memory size is equal to blockSize / warpSize < warpSize
        assert(get_n_warps_per_block() <= warpSize);
        if (wid_block == 0) {
            // We pick warp 0 to perform the warp-shuffle scan on shared memory.
            Number shm_value;
            if constexpr (std::is_floating_point_v<Number>) {
                shm_value = DEVICE_NAN;
            } else {
                shm_value = Number(0);
            }

            if (tid_warp < n_warps_per_block) {
                // Read the final value from the warp with wid_block == tid_warp
                shm_value = shm[tid_warp];
            }
            for (int subtree_size = 1, subtree_id = tid_warp;
                subtree_size < WARP_SIZE;
                subtree_size <<= 1, subtree_id /= 2) {
                const int from_lane = max(0, subtree_id * subtree_size - 1);
                const Number received_value = __shfl_sync(__activemask(), shm_value, from_lane);
                if (subtree_id % 2 == 1) {
                    const auto computed_value = Operation::apply(received_value, shm_value);
                    #ifdef DEBUG
                    if constexpr (std::is_floating_point_v<Number>) {
                        if (std::isnan(computed_value)
                            && !std::isnan(shm_value)
                            && !std::isnan(received_value)
                        ) {
                            printf("block-level operation result %f is nan when operands %f and %f\n",
                                float(computed_value), float(shm_value), float(received_value));
                        }
                    }
                    #endif // DEBUG
                    shm_value = computed_value;
                }
            }
            shm[tid_warp] = shm_value;
        }

        __syncthreads(); // Now shm contains output data from the block-level warp-shuffle scan

        // We need to read the final value of wid into wid+1 and update all the values in the warp
        // We use a shared memory broadcast to do this: each thread within a warp reads the same
        // shared memory location: shm[wid_block-1]
        if (wid_block > 0) {
            Number wid_minus_1_value = shm[wid_block-1];
            const auto computed_value = Operation::apply(wid_minus_1_value, value);
            #ifdef DEBUG
            if constexpr (std::is_floating_point_v<Number>) {
                if (std::isnan(computed_value)
                    && !std::isnan(value)
                    && !std::isnan(wid_minus_1_value)
                ) {
                    printf("propagate result %f is nan when operands %f and %f\n",
                        float(computed_value), float(value), float(wid_minus_1_value));
                }
            }
            #endif // DEBUG
            value = computed_value;
        }
        // For wid_block == 0 (first warp), value is already correct - no combination needed

        // Now the `value` variables contain the the scanned values: we need to write them back to C
        if (tid_grid < n) {
            C[tid_grid] = value;
        }
    }
}

struct Vector_scan_parallel_spec {
    const hipDeviceProp_t device_prop_;

    const std::string type_;
    const std::string operation_;

    const long m_;    // unused for vector scan
    const long n_;    // size of vector
    const long k_;    // unused for vector scan

    const long n_A_;
    const long n_C_;
    const long n_temp_;

    const dim3 block_dim_ = 0;
    const dim3 grid_dim_ = 0;
    const size_t shared_mem_size_ = 0;

    constexpr static int DEFAULT_M = 0;    // unused
    constexpr static int DEFAULT_N = 3000; // size of vector
    constexpr static int DEFAULT_K = 0;    // unused
    constexpr static int DEFAULT_BLOCK_DIM_X = 1024;

    inline static void add_kernel_spec_options(cxxopts::Options& options) {
        options.add_options()
            ("operation,op", "Operation to perform (max, min, sum, prod)", cxxopts::value<std::string>()->default_value("sum"))
            ("N", "Size of vector", cxxopts::value<long>()->default_value(std::to_string(DEFAULT_N)))
            ("block-dim", "Number of threads in the x dimension per block", cxxopts::value<unsigned>()->default_value(std::to_string(DEFAULT_BLOCK_DIM_X)))
            ("type", "Numeric type (half, single/float, double, int<n>, uint<n>)", cxxopts::value<std::string>()->default_value("float"));
        ;
    }

    inline static Vector_scan_parallel_spec make(
        const cxxopts::ParseResult& options_parsed
    ) {
        // Validate the type option
        const auto& type = options_parsed["type"].as<std::string>();
        if (type != "half" && type != "single" && type != "float" && type != "double" && type != "int8" && type != "int16" && type != "int32" && type != "int64" && type != "uint8" && type != "uint16" && type != "uint32" && type != "uint64") {
            std::cerr << "[ERROR] --type must be one of: half, single/float, double, int<n>, uint<n>" << std::endl;
            throw cxxopts::exceptions::exception("Invalid --type: " + type);
        }
        const auto& operation = options_parsed["operation"].as<std::string>();
        if (operation != "max" && operation != "min" && operation != "sum" && operation != "prod") {
            std::cerr << "[ERROR] --operation must be one of: max, min, sum, prod" << std::endl;
            throw cxxopts::exceptions::exception("Invalid --operation: " + operation);
        }
        const auto size = options_parsed["N"].as<long>();
        const auto block_dim_option = options_parsed["block-dim"].as<unsigned>();
        const auto warp_size = get_warp_size();
        const auto block_size = (std::min(size, (long)block_dim_option)  + warp_size - 1) / warp_size * warp_size;
        return make(
            type,
            operation,
            size,
            block_size
        );
    }

    static int compute_size_of_temp(const int n_elems, const int block_size) {
        assert(block_size > 1);
        if (n_elems <= block_size) {
            return 0;
        }
        int size_temp = 1;
        for(int n_elems_remaining = (n_elems + block_size - 1) / block_size;
            n_elems_remaining > 1;
            n_elems_remaining = (n_elems_remaining + block_size - 1) / block_size
        ) {
            size_temp += n_elems_remaining;
        }
        const int n_blocks = (size_temp + block_size - 1) / block_size;
        return n_blocks * block_size;
    }

    inline static Vector_scan_parallel_spec make(
        const auto& type,
        const auto& operation,
        const long size,
        const long block_size
    ) {
        const auto [scalar_size, hip_occupancy_kernel]  = (
            type == "half" ? std::make_tuple(sizeof(__half), (void*)vector_scan_by_blocks_parallel<half, hip_sum_op<half>>) :
            type == "single" || type == "float" ? std::make_tuple(sizeof(float), (void*)vector_scan_by_blocks_parallel<float, hip_sum_op<float>>) :
            type == "double" ? std::make_tuple(sizeof(double), (void*)vector_scan_by_blocks_parallel<double, hip_sum_op<double>>) :
            type == "int8" ? std::make_tuple(sizeof(int8_t), (void*)vector_scan_by_blocks_parallel<int8_t, hip_sum_op<int8_t>>) :
            type == "int16" ? std::make_tuple(sizeof(int16_t), (void*)vector_scan_by_blocks_parallel<int16_t, hip_sum_op<int16_t>>) :
            type == "int32" ? std::make_tuple(sizeof(int32_t), (void*)vector_scan_by_blocks_parallel<int32_t, hip_sum_op<int32_t>>) :
            type == "int64" ? std::make_tuple(sizeof(int64_t), (void*)vector_scan_by_blocks_parallel<int64_t, hip_sum_op<int64_t>>) :
            type == "uint8" ? std::make_tuple(sizeof(uint8_t), (void*)vector_scan_by_blocks_parallel<uint8_t, hip_sum_op<uint8_t>>) :
            type == "uint16" ? std::make_tuple(sizeof(uint16_t), (void*)vector_scan_by_blocks_parallel<uint16_t, hip_sum_op<uint16_t>>) :
            type == "uint32" ? std::make_tuple(sizeof(uint32_t), (void*)vector_scan_by_blocks_parallel<uint32_t, hip_sum_op<uint32_t>>) :
            type == "uint64" ? std::make_tuple(sizeof(uint64_t), (void*)vector_scan_by_blocks_parallel<uint64_t, hip_sum_op<uint64_t>>) :
            std::make_tuple(0, nullptr)
        );
        if (hip_occupancy_kernel == nullptr) {
            std::cerr << "[ERROR] Invalid type: " << type << std::endl;
            throw cxxopts::exceptions::exception("Invalid type: " + type);
        }
        return make(type, operation, scalar_size, hip_occupancy_kernel, size, block_size);
    }

    template <HIP_scalar Number_, typename Operation_= hip_sum_op<Number_>>
    inline static Vector_scan_parallel_spec make(
        const long size,
        const long block_size
    ) {
        const auto scalar_size = sizeof(Number_);
        const auto hip_occupancy_kernel = vector_scan_by_blocks_parallel<Number_, Operation_>;
        return make(type_name(type_t<Number_>{}), Operation_::name, scalar_size, (void*)hip_occupancy_kernel, size, block_size);
    }

    protected:
    inline static Vector_scan_parallel_spec make(
        const auto& type,
        const auto& operation,
        const size_t scalar_size,
        const void* const hip_occupancy_kernel,
        const long size,
        const long block_size
    ) {
        if (block_size < 1) {
            throw std::invalid_argument("block_size must be at least 1");
        };
        if (hip_occupancy_kernel == nullptr) {
            throw std::invalid_argument("hip_occupancy_kernel must not be nullptr");
        };
        if (scalar_size <= 0) {
            throw std::invalid_argument("scalar_size must be positive");
        };

        int max_block_size = 0;
        int opt_grid_size = 0;
        int max_active_blocks_per_multiprocessor = 0;
        const auto shared_mem_guess = (compute_n_warps_per_block(block_size/4)+1) * scalar_size;
        hip_check_error(hipOccupancyMaxPotentialBlockSize(
            &opt_grid_size,
            &max_block_size,
            hip_occupancy_kernel,
            shared_mem_guess,
            block_size
        ), "hipOccupancyMaxPotentialBlockSize");

        // opt_grid_size is a good-to-know number, but we have to compute grid_dim based on the size of vector
        const auto grid_dim = dim3((size+max_block_size-1)/max_block_size);
        const auto block_dim = dim3(max_block_size);
        const auto shared_mem_size = (compute_n_warps_per_block(max_block_size)+1) * scalar_size;
        const auto n_temp = compute_size_of_temp(size, max_block_size);
        hip_check_error(hipOccupancyMaxActiveBlocksPerMultiprocessor(
            &max_active_blocks_per_multiprocessor,
            hip_occupancy_kernel,
            block_dim.x,
            shared_mem_size
        ), "hipOccupancyMaxActiveBlocksPerMultiprocessor");
        std::cout << "[INFO] max_active_blocks_per_multiprocessor: " << max_active_blocks_per_multiprocessor
                  << " at block_size:" << max_block_size << std::endl;
        std::cout << "[INFO] max_block_size: " << max_block_size << std::endl;
        std::cout << "[INFO] opt_grid_size: " << opt_grid_size << std::endl;
        std::cout << "[INFO] grid_dim: " << grid_dim.x << ", " << grid_dim.y << ", " << grid_dim.z << std::endl;
        std::cout << "[INFO] block_dim: " << block_dim.x << ", " << block_dim.y << ", " << block_dim.z << std::endl;
        std::cout << "[INFO] shared_mem_size: " << shared_mem_size << std::endl;
        return Vector_scan_parallel_spec(type, operation, size, block_dim, grid_dim, shared_mem_size, n_temp);
    }

    protected:
    Vector_scan_parallel_spec(
        const auto& type,
        const auto& operation,
        const long size,
        const dim3 block_dim,
        const dim3 grid_dim,
        const size_t shared_mem_size,
        const int n_temp
    ) : device_prop_(get_default_device_prop()),
        type_(type),
        operation_(operation),  // Store the actual operation string
        m_(0),  // unused
        n_(size),
        k_(0),  // unused
        n_A_(size),
        n_C_(size),
        n_temp_(n_temp),
        block_dim_(block_dim),
        grid_dim_(grid_dim),
        shared_mem_size_(shared_mem_size)
    {}
};

static_assert(Check_vector_kernel_spec_1In_1Out<Vector_scan_parallel_spec>::check_passed, "Vector_scan_parallel_spec is not a valid kernel spec");


template <HIP_scalar Number_, typename Operation_= hip_sum_op<Number_>>
class Vector_scan_parallel_kernel {
    public:
    using Number = Number_;
    using Operation = Operation_;
    using Kernel_spec = Vector_scan_parallel_spec;

    const Kernel_spec spec_;

    Vector_scan_parallel_kernel(
        const Kernel_spec spec
    ) : spec_(spec) {}

    void block_strided_pass(
        Number* const input_buffer,
        const int prev_result_index,
        const int prev_n_elems,
        Number* const output_buffer,
        const int curr_result_index,
        hipStream_t stream
    ) {
        Number* const prev_result = input_buffer + prev_result_index;
        const int prev_n_blocks = (prev_n_elems + spec_.block_dim_.x - 1)/spec_.block_dim_.x;
        Number* const curr_result = output_buffer + curr_result_index;
        const int curr_n_elems = prev_n_blocks;
        const int curr_n_blocks = (curr_n_elems + spec_.block_dim_.x - 1)/spec_.block_dim_.x;
        if (curr_n_elems > 1) {
            vector_scan_by_blocks_parallel<Number, Operation><<<curr_n_blocks, spec_.block_dim_, spec_.shared_mem_size_, stream>>>(
                prev_result,
                curr_result,
                curr_n_elems,
                spec_.block_dim_.x,
                prev_n_elems
            );
            if (prev_n_elems > static_cast<int>(spec_.block_dim_.x)) {
                const auto next_result_index = curr_result_index + curr_n_elems;
                block_strided_pass(output_buffer, curr_result_index, curr_n_elems, output_buffer, next_result_index, stream);
                // By the powers of recursion, curr_result is fully scanned
                // Now we have to compute the deltas between curr_result and the final elements
                // of each stride of prev_result, and apply those delta retroactively to the strides
                // of prev_result

                vector_scan_write_back_parallel<Number, Operation><<<prev_n_blocks-1, spec_.block_dim_, 0, stream>>>(
                    prev_result,
                    prev_n_elems,
                    curr_result,
                    curr_n_elems,
                    spec_.block_dim_.x
                );
            }
        }
    }

    void run_device_kernel(
        const Number* const gpu_data_A,
        Number* const gpu_data_C,
        Number* const gpu_data_temp,
        hipStream_t stream
    ) {
        // In our downward iteration we start by processing A block-wise.
        // The produces in C an array of block-wise scans that we can further process with a stride = block_size.
        // This downward iteration ends when the number of blocks is 1, which means that the result is the
        // global scan.
        vector_scan_by_blocks_parallel<Number, Operation><<<spec_.grid_dim_, spec_.block_dim_, spec_.shared_mem_size_, stream>>>(
            gpu_data_A,
            gpu_data_C,
            spec_.n_,
            1
        );

        auto& prev_result = gpu_data_C;
        auto& curr_result = gpu_data_temp;
        auto& prev_n_elems = spec_.n_;
        if (prev_n_elems > static_cast<int>(spec_.block_dim_.x)) {
            block_strided_pass(prev_result, 0, spec_.n_, curr_result, 0, stream);
        }
    }

    Eigen::Vector<Number, Eigen::Dynamic> run_host_kernel(
        const Eigen::Map<Eigen::Vector<Number, Eigen::Dynamic>>& A
    ) {
        // Compute cumulative max for a vector (treat matrix as a flattened vector)
        Eigen::Vector<Number, Eigen::Dynamic> result(A.rows(), A.cols());
        Number accu = A(0);
        result(0) = accu;
        for (long i = 1; i < A.size(); ++i) {
            accu = Operation::apply(A(i),accu);
            result(i) = accu;
        }
        return result;
    }
};

static_assert(Check_vector_kernel_1In_1Out_template<Vector_scan_parallel_kernel>::check_passed, "Vector_scan_parallel_kernel is not a valid kernel template");
