// Copyright (c) 2025 Alessandro Baretta
// All rights reserved.

// source path: include/hip/kernels/matrix/matrix_product_tensor.hpp

#pragma once

#include <iostream>

#include <hip/hip_runtime.h>
#include <rocwmma/rocwmma.hpp>
#include <cxxopts.hpp>
#include <Eigen/Dense>
#include <type_traits>
#include <cstdint>
#include <hip/hip_fp16.h>

#include "hip/kernel_api/matrix_2in_1out.hiph"
#include "hip/type_traits.hiph"

struct Matrix_product_tensor_spec {
    const std::string type_;

    const long m_;    // Rows of first matrix
    const long n_;    // Columns of first matrix and rows of second matrix
    const long k_;    // Columns of second matrix

    const long n_rows_A_;
    const long n_cols_A_;

    const long n_rows_B_;
    const long n_cols_B_;

    const long n_rows_C_;
    const long n_cols_C_;

    const long n_rows_temp_;
    const long n_cols_temp_;

    const dim3 block_dim_;
    const dim3 grid_dim_;
    const size_t dynamic_shared_mem_words_ = 0;

    constexpr static int DEFAULT_M = 3000; // Rows of first matrix
    constexpr static int DEFAULT_N = 300;  // Columns of first matrix / Rows of second matrix
    constexpr static int DEFAULT_K = 1000; // Columns of second matrix
    constexpr static int DEFAULT_BLOCK_DIM_X = 16;
    constexpr static int DEFAULT_BLOCK_DIM_Y = 16;

    inline static void add_kernel_spec_options(cxxopts::Options& options) {
        options.add_options()
            ("M", "Number of rows in first matrix", cxxopts::value<long>()->default_value(std::to_string(DEFAULT_M)))
            ("N", "Number of columns in first matrix and rows of the second matrix", cxxopts::value<long>()->default_value(std::to_string(DEFAULT_N)))
            ("K", "Number of columns in the second matrix", cxxopts::value<long>()->default_value(std::to_string(DEFAULT_K)))
            ("block-dim-x", "Number of threads in the x dimension per block", cxxopts::value<unsigned>()->default_value(std::to_string(DEFAULT_BLOCK_DIM_X)))
            ("block-dim-y", "Number of threads in the y dimension per block", cxxopts::value<unsigned>()->default_value(std::to_string(DEFAULT_BLOCK_DIM_Y)))
            ("type", "Numeric type (half*, int8*, single/float, double, int16, int32, int64, uint8, uint16, uint32, uint64) (* = tensor cores)", cxxopts::value<std::string>()->default_value("float"));
        ;
    }

    inline static Matrix_product_tensor_spec make(
        const cxxopts::ParseResult& options_parsed
    ) {
        // Validate the type option - now accepts all types supported by the main program
        const auto& type = options_parsed["type"].as<std::string>();
        if (type != "half" && type != "single" && type != "float" && type != "double" &&
            type != "int8" && type != "int16" && type != "int32" && type != "int64" &&
            type != "uint8" && type != "uint16" && type != "uint32" && type != "uint64") {
            std::cerr << "[ERROR] --type must be one of: half, single/float, double, int8, int16, int32, int64, uint8, uint16, uint32, uint64" << std::endl;
            throw cxxopts::exceptions::exception("Invalid --type: " + type);
        }
        return Matrix_product_tensor_spec(
            type,
            options_parsed["M"].as<long>(),
            options_parsed["N"].as<long>(),
            options_parsed["K"].as<long>(),
            options_parsed["block-dim-x"].as<unsigned>(),
            options_parsed["block-dim-y"].as<unsigned>()
        );
    }

    inline Matrix_product_tensor_spec(
        const std::string& type,
        const long m,
        const long n,
        const long k,
        const long block_dim_x,
        const long block_dim_y
    ) : type_(type),
        m_(m),
        n_(n),
        k_(k),
        n_rows_A_(m),
        n_cols_A_(n),
        n_rows_B_(n),
        n_cols_B_(k),
        n_rows_C_(m),
        n_cols_C_(k),
        n_rows_temp_(0),
        n_cols_temp_(0),
        block_dim_(block_dim_x, block_dim_y),
        grid_dim_(
            // For small matrices, use thread-per-element grid for naive implementation
            // For large matrices, use WMMA tile-based grid
            (k_ < 16 || m_ < 16) ?
                dim3((k_ + block_dim_x - 1) / block_dim_x, (m_ + block_dim_y - 1) / block_dim_y) :
                dim3((k_ + 15) / 16, (m_ + 15) / 16)
        )
    {}
};

static_assert(Check_matrix_kernel_spec_2In_1Out<Matrix_product_tensor_spec>::check_passed, "Matrix_product_tensor_spec is not a valid kernel spec");

// WMMA tensor core matrix multiplication kernel
template <typename Number>
__global__ void matrix_product_tensor_wmma(
    const Number* A,
    const Number* B,
    Number* C,
    const int m,
    const int n,
    const int k
) {
    // WMMA fragment dimensions
    const int WMMA_M = 16;
    const int WMMA_N = 16;
    const int WMMA_K = 16;

    // Shared memory for int8 conversion
    __shared__ int shared_temp[WMMA_M * WMMA_N];

    // Calculate which 16x16 tile this block handles
    const int block_row = blockIdx.y;
    const int block_col = blockIdx.x;

    // Each block handles one 16x16 output tile
    const int row_base = block_row * WMMA_M;
    const int col_base = block_col * WMMA_N;

    if constexpr (std::is_same_v<Number, __half>) {
        // For matrices smaller than WMMA tile size, fall back to naive implementation
        if (m < WMMA_M || n < WMMA_K || k < WMMA_N) {
            // Fallback to naive implementation for small matrices
            const int row = blockIdx.y * blockDim.y + threadIdx.y;
            const int col = blockIdx.x * blockDim.x + threadIdx.x;

            if (row < m && col < k) {
                __half sum = 0;
                for (int i = 0; i < n; ++i) {
                    sum += A[row * n + i] * B[i * k + col];
                }
                C[row * k + col] = sum;
            }
            return;
        }

        using namespace rocwmma;

        // Get wave ID within the block (AMD equivalent of warp)
        const int waveId = (threadIdx.y * blockDim.x + threadIdx.x) / warpSize; // Use builtin warpSize

        // For small matrices (single block), only first wave processes the entire matrix
        // For large matrices, each block processes one tile
        if (waveId == 0) {
            // Declare WMMA fragments
            fragment<matrix_a, WMMA_M, WMMA_N, WMMA_K, __half, row_major> a_frag;
            fragment<matrix_b, WMMA_M, WMMA_N, WMMA_K, __half, row_major> b_frag;
            fragment<accumulator, WMMA_M, WMMA_N, WMMA_K, __half> c_frag;

            // Initialize accumulator to zero
            fill_fragment(c_frag, 0.0f);

            // For small matrices in single block mode, process from (0,0)
            int effective_row_base = (gridDim.x == 1 && gridDim.y == 1) ? 0 : row_base;
            int effective_col_base = (gridDim.x == 1 && gridDim.y == 1) ? 0 : col_base;

            // Loop over K dimension in chunks of WMMA_K
            for (int i = 0; i < n; i += WMMA_K) {
                // Check if we have enough elements in this K-chunk or if this is the last chunk
                int k_chunk_size = min(WMMA_K, n - i);

                // Only proceed if we have data to process and our tile intersects with the matrix
                if (effective_row_base < m && effective_col_base < k && k_chunk_size > 0) {
                    // Load fragments from global memory with bounds checking
                    // WMMA will handle partial tiles internally through leading dimension
                    load_matrix_sync(a_frag, A + effective_row_base * n + i, n);
                    load_matrix_sync(b_frag, B + i * k + effective_col_base, k);

                    // Perform tensor core matrix multiplication
                    mma_sync(c_frag, a_frag, b_frag, c_frag);
                }
            }

            // Store the result - only need to check if our tile intersects with the output matrix
            if (effective_row_base < m && effective_col_base < k) {
                // For small matrices, we need to be careful about partial tile storage
                // store_matrix_sync can overwrite adjacent memory with large strides
                if (effective_row_base + WMMA_M <= m && effective_col_base + WMMA_N <= k) {
                    // Full tile fits, safe to use direct store
                    store_matrix_sync(C + effective_row_base * k + effective_col_base, c_frag, k, mem_row_major);
                } else {
                    // Partial tile - need element-wise extraction and storage
                    __half temp_result[WMMA_M * WMMA_N];
                    store_matrix_sync(temp_result, c_frag, WMMA_N, mem_row_major);

                    // Copy only the valid elements
                    for (int i = 0; i < WMMA_M && effective_row_base + i < m; ++i) {
                        for (int j = 0; j < WMMA_N && effective_col_base + j < k; ++j) {
                            C[(effective_row_base + i) * k + (effective_col_base + j)] = temp_result[i * WMMA_N + j];
                        }
                    }
                }
            }
        }
    } else if constexpr (std::is_same_v<Number, std::int8_t>) {
        // For matrices smaller than WMMA tile size, fall back to naive implementation
        if (m < WMMA_M || n < WMMA_K || k < WMMA_N) {
            // Fallback to naive implementation for small matrices
            const int row = blockIdx.y * blockDim.y + threadIdx.y;
            const int col = blockIdx.x * blockDim.x + threadIdx.x;

            if (row < m && col < k) {
                Number sum = 0;
                for (int i = 0; i < n; ++i) {
                    sum += A[row * n + i] * B[i * k + col];
                }
                C[row * k + col] = sum;
            }
            return;
        }

        using namespace rocwmma;

        // Get wave ID within the block (AMD equivalent of warp)
        const int waveId = (threadIdx.y * blockDim.x + threadIdx.x) / warpSize; // Use builtin warpSize

        // Only first wave per block performs WMMA operations
        if (waveId == 0) {
            // Declare ROCWMMA fragments for INT8 inputs with INT8 accumulation
            fragment<matrix_a, WMMA_M, WMMA_N, WMMA_K, int8_t, row_major> a_frag;
            fragment<matrix_b, WMMA_M, WMMA_N, WMMA_K, int8_t, row_major> b_frag;
            fragment<accumulator, WMMA_M, WMMA_N, WMMA_K, int8_t> c_frag;

            // Initialize accumulator to zero
            fill_fragment(c_frag, 0);

            // For small matrices in single block mode, process from (0,0)
            int effective_row_base = (gridDim.x == 1 && gridDim.y == 1) ? 0 : row_base;
            int effective_col_base = (gridDim.x == 1 && gridDim.y == 1) ? 0 : col_base;

            // Loop over K dimension in chunks of WMMA_K
            for (int i = 0; i < n; i += WMMA_K) {
                // Check if we have enough elements in this K-chunk or if this is the last chunk
                int k_chunk_size = min(WMMA_K, n - i);

                // Only proceed if we have data to process and our tile intersects with the matrix
                if (effective_row_base < m && effective_col_base < k && k_chunk_size > 0) {
                    // Load fragments from global memory with bounds checking
                    load_matrix_sync(a_frag,
                        reinterpret_cast<const int8_t*>(A) + effective_row_base * n + i, n);
                    load_matrix_sync(b_frag,
                        reinterpret_cast<const int8_t*>(B) + i * k + effective_col_base, k);

                    // Perform tensor core matrix multiplication
                    mma_sync(c_frag, a_frag, b_frag, c_frag);
                }
            }

            // Store the result
            if (effective_row_base < m && effective_col_base < k) {
                // For small matrices, we need to be careful about partial tile storage
                // store_matrix_sync can overwrite adjacent memory with large strides
                if (effective_row_base + WMMA_M <= m && effective_col_base + WMMA_N <= k) {
                    // Full tile fits, safe to use direct store to shared memory first
                    store_matrix_sync(shared_temp, c_frag, WMMA_N, mem_row_major);

                    // Synchronize threads within the block
                    __syncthreads();

                    // Copy to output
                    for (int i = 0; i < WMMA_M; ++i) {
                        for (int j = 0; j < WMMA_N; ++j) {
                            int val = shared_temp[i * WMMA_N + j];
                            // Use proper integer overflow semantics instead of clamping
                            // Cast to int8_t allows natural overflow: 127+1 => -128
                            C[(effective_row_base + i) * k + (effective_col_base + j)] = static_cast<std::int8_t>(val);
                        }
                    }
                } else {
                    // Partial tile - store to shared memory first then copy selectively
                    store_matrix_sync(shared_temp, c_frag, WMMA_N, mem_row_major);

                    // Synchronize threads within the block
                    __syncthreads();

                    // Convert and store only valid elements
                    for (int i = 0; i < WMMA_M && effective_row_base + i < m; ++i) {
                        for (int j = 0; j < WMMA_N && effective_col_base + j < k; ++j) {
                            // Use proper integer overflow semantics instead of clamping
                            int val = shared_temp[i * WMMA_N + j];
                            C[(effective_row_base + i) * k + (effective_col_base + j)] = static_cast<std::int8_t>(val);
                        }
                    }
                }
            }
        }
    } else {
        // Fallback to naive implementation for unsupported types
        const int row = blockIdx.y * blockDim.y + threadIdx.y;
        const int col = blockIdx.x * blockDim.x + threadIdx.x;

        if (row < m && col < k) {
            Number sum = 0;
            for (int i = 0; i < n; ++i) {
                sum += A[row * n + i] * B[i * k + col];
            }
            C[row * k + col] = sum;
        }
    }
}

template <HIP_scalar Number_>
class Matrix_product_tensor_kernel {
    public:
    using Number = Number_;
    using Kernel_spec = Matrix_product_tensor_spec;

    const Kernel_spec spec_;
    Matrix_product_tensor_kernel(
        const Kernel_spec spec
    ) : spec_(spec) {}

    void run_device_kernel(
        const Number* const gpu_data_A,
        const Number* const gpu_data_B,
        Number* const gpu_data_C,
        Number* const gpu_data_temp,
        hipStream_t stream
    ) {
        // Launch tensor core matrix multiplication kernel
        matrix_product_tensor_wmma<<<
            spec_.grid_dim_,
            spec_.block_dim_,
            spec_.dynamic_shared_mem_words_ * sizeof(Number),
            stream
        >>>(gpu_data_A, gpu_data_B, gpu_data_C, spec_.m_, spec_.n_, spec_.k_);
    }
    Eigen::Matrix<Number, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor> run_host_kernel(
        const Eigen::Map<Eigen::Matrix<Number, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>>& A,
        const Eigen::Map<Eigen::Matrix<Number, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>>& B
    ) {
        return (A * B).eval();
    }

};
static_assert(Check_matrix_kernel_2In_1Out_template<Matrix_product_tensor_kernel>::check_passed, "Matrix_product_tensor is not a valid kernel template");
