/*
    Copyright (c) 2025 Alessandro Baretta <alex@baretta.com>

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.
*/


// source path: include/hip/hip_utils.hiph

#pragma once

#include <hip/hip_runtime.h>
#include <hip/hip_fp16.h>
#include "check_errors.hiph"
#include "type_traits.hiph"

#if __cplusplus <= 202002L
namespace std {
    using float16_t = _Float16;
}
#endif

constexpr size_t NULL_FLAGS = 0;
constexpr static long MAX_BLOCK_SIZE = 1024;
constexpr static unsigned long FULL_MASK = std::numeric_limits<unsigned>::max();

#define WARP_SIZE warpSize
#define MAX_N_WARPS (MAX_BLOCK_SIZE / WARP_SIZE)
#define LAST_LANE (WARP_SIZE - 1)

__host__ __inline__
int get_warp_size() {
    hipDeviceProp_t props;
    hip_check_error(hipGetDeviceProperties(&props, 0), "hipGetDeviceProperties");
    return props.warpSize;
}
__host__ __inline__
unsigned int compute_n_threads_per_block(const dim3 block_dim) {
    return long(block_dim.x) * long(block_dim.y) * long(block_dim.z);
}
__device__ __inline__
unsigned int compute_n_threads_per_block() {
    return long(blockDim.x) * long(blockDim.y) * long(blockDim.z);
}

template <std::integral T, std::integral U>
__host__ __inline__
unsigned int compute_n_warps_per_block(const T n_threads_per_block, const U warp_size = get_warp_size()) {
    return (n_threads_per_block + warp_size - 1) / warp_size;
}

__host__ __inline__
unsigned int compute_n_warps_per_block(const dim3& block_dim, const int warp_size = get_warp_size()) {
    return compute_n_warps_per_block(compute_n_threads_per_block(dim3(block_dim)), warp_size);
}

__device__ __inline__
unsigned int get_n_warps_per_block(const long n_threads_per_block) {
    return (n_threads_per_block + warpSize - 1) / warpSize;
}
__device__ __inline__
unsigned int get_n_warps_per_block() {
    return (compute_n_threads_per_block() + warpSize - 1) / warpSize;
}


__host__ __device__ __inline__
void* align_pointer(void *ptr, const std::size_t alignment) {
    const std::size_t ptr_value = reinterpret_cast<size_t>(ptr);
    const std::size_t aligned_ptr_value = (ptr_value + (alignment - 1)) & ~(alignment - 1);
    void* const aligned_ptr = reinterpret_cast<void*>(aligned_ptr_value);
    return aligned_ptr;
}

// Device function to get dynamic shared memory pointer - non-template to avoid symbol issues
// We cannot declare dynamic_shm aligned with __aligned__(alignof(T)) because this would require
// templatizing this function, which causes linker errors.
__device__ __inline__
void* get_dynamic_shared_memory(const std::size_t alignment) {
    extern __shared__ void* dynamic_shm[];
    void* aligned_shm = align_pointer(dynamic_shm, alignment);
    return aligned_shm;
}


template <HIP_scalar Number>
__host__ __device__ Number hip_max(Number a, Number b) {
    return max(a, b);
}

// Template specialization definition (inline to avoid linker issues)
template <>
__host__ __device__ inline __half hip_max<__half>(__half a, __half b) {
    return fmaxf(float(a), float(b));
}

template <HIP_scalar Number>
__host__ __device__ Number hip_min(Number a, Number b) {
    return min(a, b);
}

// Template specialization definition (inline to avoid linker issues)
template <>
__host__ __device__ inline __half hip_min<__half>(__half a, __half b) {
    return fminf(float(a), float(b));
}

template <HIP_scalar Number>
__host__ __device__ Number hip_sum(Number a, Number b) {
    return a + b;
}

template <HIP_scalar Number>
__host__ __device__ Number hip_prod(Number a, Number b) {
    return a * b;
}

template <HIP_scalar Number>
__host__ __device__ Number device_nan() {
    static_assert(std::is_floating_point_v<Number>, "device_nan only supports floating point types");
    if constexpr (std::is_same_v<Number, float>) {
        return nanf("");
    } else if constexpr (std::is_same_v<Number, double>) {
        return nan("");
    }
    // __half specialization is defined below
}

// Template specialization definition (inline to avoid linker issues)
template <>
__host__ __device__ inline __half device_nan<__half>() {
    return nanf("");
}

void report_completion_time_callback(hipStream_t stream, hipError_t status, void* userData);

// Function objects for HIP operations - these work reliably in device code
template <typename Number_>
struct hip_max_op {
    using Number = Number_;
    __host__ __device__ static Number apply(Number a, Number b) {
        return max(a, b);
    }

    __host__ __device__ static Number identity() {
        if constexpr (std::is_floating_point_v<Number>) {
            return -INFINITY;
        } else {
            return Number(1) << (sizeof(Number) * 8 - 1);
        }
    }
};

template <>
struct hip_max_op<__half> {
    using Number = __half;
    __host__ __device__ static __half apply(__half a, __half b) {
        return __hmax(a, b);
    }

    __host__ __device__ static __half identity() {
        return __hmax(__hneg(0x7c00), __hneg(0x7c00));
    }
};

template <typename Number_>
struct hip_min_op {
    using Number = Number_;
    __host__ __device__ static Number apply(Number a, Number b) {
        return min(a, b);
    }

    __host__ __device__ static Number identity() {
        if constexpr (std::is_floating_point_v<Number>) {
            return INFINITY;
        } else {
            return ~(Number(1) << (sizeof(Number) * 8 - 1));
        }
    }
};

template <>
struct hip_min_op<__half> {
    using Number = __half;
    __host__ __device__ static __half apply(__half a, __half b) {
        return __hmin(a, b);
    }

    __host__ __device__ static __half identity() {
        return __hmin(__hneg(0x7c00), __hneg(0x7c00));
    }
};

template <typename Number_>
struct hip_sum_op {
    using Number = Number_;
    __host__ __device__ static Number apply(Number a, Number b) {
        return a + b;
    }

    __host__ __device__ static Number identity() {
        return Number(0);
    }
};

template <typename Number_>
struct hip_prod_op {
    using Number = Number_;
    __host__ __device__ static Number apply(Number a, Number b) {
        return a * b;
    }

    __host__ __device__ static Number identity() {
        return Number(1);
    }
};

hipDeviceProp_t get_device_prop(const int device_id);
hipDeviceProp_t get_default_device_prop();
